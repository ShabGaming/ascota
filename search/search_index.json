{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ASCOTA","text":"<p>ASCOTA (Archaeological Sherd Color, Texture &amp; Analysis) is a research framework for the computational study of archaeological ceramics. The project develops methods for image standardization, segmentation, classification, and measurement to support archaeological research at The University of Hong Kong (HKU).</p> <p>Manual labeling and measurement of terabytes of excavation images is slow and labor-intensive. ASCOTA aims to automate the backlog of image processing, standardize new field data, and provide tools for archaeologists to analyze and classify sherds more efficiently.</p>"},{"location":"#about","title":"About","text":"<ul> <li>Project Created By: M. Shahab Hasan </li> <li>University Supervisor: Dr. Peter J. Cobb (School of Humanities, HKU)</li> <li>Research Context: Archaeological fieldwork under APSAP (Ararat Plain Southeast Archaeological Project)</li> </ul>"},{"location":"#pipeline-overview","title":"Pipeline Overview","text":"<p>ASCOTA is organized into three phases, with modular Python packages and tools for each stage:</p>"},{"location":"#1-phase-1-core-image-processing-completed","title":"1. \ud83d\udccc Phase 1 \u2014 Core Image Processing (\u2705 Completed)","text":"<ul> <li>Module: <code>ascota_core</code> </li> <li>Purpose: Standardize and prepare raw excavation images.  </li> <li>Functions: </li> <li>Segmentation &amp; ROI detection (using Segmentation + OpenCV)  </li> <li>Color card detection, classification, and color correction </li> <li>Pixel-to-centimeter scale estimation using measurement cards  </li> <li>Surface area estimation of sherds  </li> </ul> <p>\u27a1 Outputs: clean, standardized, and calibrated images ready for classification.</p>"},{"location":"#2-phase-2-classification-labeling-in-progress","title":"2. \ud83d\udccc Phase 2 \u2014 Classification &amp; Labeling (\ud83d\udea7 In Progress)","text":"<ul> <li>Goal: Automatically classify and label pottery sherds.  </li> <li>Techniques: </li> <li>Deep learning neural networks  </li> <li>Vision Transformers (ViTs)  </li> <li>Computer Vision models for object detection and pattern recognition  </li> <li>Targets: </li> <li>Sherd type (e.g., rim, base, body fragment)  </li> <li>Color range classification  </li> <li>Texture detection  </li> <li>Pattern/design recognition  </li> </ul>"},{"location":"#3-phase-3-end-to-end-automation-ui-planned","title":"3. \ud83d\udccc Phase 3 \u2014 End-to-End Automation &amp; UI (\ud83d\uddd3\ufe0f Planned)","text":"<ul> <li>Goal: Deliver an accessible tool for archaeologists and non-technical users.  </li> <li>Planned Outputs: </li> <li>End-to-end automation scripts  </li> <li>A user-friendly Streamlit-based or desktop application  </li> <li>Generalized framework that works beyond APSAP contexts  </li> </ul>"},{"location":"#repository-structure","title":"Repository Structure","text":"<pre><code>ASCOTA/\n\u251c\u2500 src/                     # Core source code\n\u2502  \u251c\u2500 ascota_core/          # Phase 1: segmentation, color, scale modules\n\u2502  \u251c\u2500 ascota_classification/ # (planned) Phase 2: classification models\n\u2502  \u2514\u2500 ascota_ui/            # (planned) user interface and apps\n\u251c\u2500 tests/                   # Unit tests, sample data &amp; Streamlit demo apps\n\u251c\u2500 docs/                    # MkDocs + Material documentation site\n\u251c\u2500 README.md                # Project overview (this file)\n\u2514\u2500 requirements.txt         # Dependencies\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>Full documentation is built with MkDocs + Material. It includes API reference (via <code>mkdocstrings</code>), tutorials, and research notes.</p> <pre><code>mkdocs serve\n</code></pre> <p>Open locally on your browser at http://localhost:8000 or view online at https://shabgaming.github.io/ascota.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Clone and install dependencies:</p> <pre><code>git clone https://github.com/ShabGaming/ascota.git\ncd ASCOTA\npython -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\npip install -r requirements.txt\n</code></pre> <p>Run a demo Streamlit app:</p> <pre><code>streamlit run tests/streamlits/color_correct_streamlit.py # Demo app for color correction\n</code></pre>"},{"location":"#research-vision","title":"Research Vision","text":"<p>ASCOTA is more than a software project\u2014it is part of a research initiative to advance archaeological methods with AI-powered image analysis:</p> <ul> <li>Reduce manual labor in sherd classification</li> <li>Make excavation data more searchable and analyzable</li> <li>Provide reproducible, standardized digital workflows</li> <li>Bridge the gap between AI research and archaeological practice</li> </ul> <p>This work will contribute to academic publication in collaboration with Dr. Peter J. Cobb and the APSAP project.</p>"},{"location":"#license","title":"License","text":"<p>MIT License \u2014 Open for academic and research use.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<ul> <li>APSAP Project for providing excavation data and research context.</li> <li>HKU Applied AI Program for academic support.</li> <li>Segment Anything Model (SAM), OpenCV, briaai/RMBG-1.4, and other open-source tools   powering this work.</li> </ul>"},{"location":"apps/","title":"Streamlit Apps","text":"<p>ASCOTA ships with a set of Streamlit-based demo applications. These apps provide a simple UI for archaeologists and researchers to test the core functions interactively \u2014 no coding required.</p> <p>Each app wraps functions from <code>ascota_core</code> to demonstrate the Phase 1 pipeline (capabilities for segmentation, scale estimation, and color correction).</p>"},{"location":"apps/#how-to-run","title":"How to run","text":"<p>From the project root:</p> <pre><code># Activate environment first\nsource .venv/bin/activate   # Windows: .venv\\Scripts\\activate\n\n# Then run any of the apps:\nstreamlit run tests/streamlits/color_correct_streamlit.py\nstreamlit run tests/streamlits/scale_streamlit.py\nstreamlit run tests/streamlits/segment_streamlit.py\n</code></pre> <p>Open your browser at http://localhost:8501.</p>"},{"location":"apps/#available-apps","title":"Available Apps","text":""},{"location":"apps/#color-correction","title":"\ud83c\udfa8 Color Correction","text":"<p>File: <code>color_correct_streamlit.py</code></p> <ul> <li>Detects color reference cards in input images.</li> <li>Applies color correction to normalize pottery sherd images to a selected target image.</li> <li>Useful for ensuring consistent analysis across lighting conditions and excavation seasons.</li> <li>Allows testing of various color correction/transformation algorithms.</li> </ul>"},{"location":"apps/#scale-estimation","title":"\ud83d\udccf Scale Estimation","text":"<p>File: <code>scale_streamlit.py</code></p> <ul> <li>Detects measurement cards in excavation images.</li> <li>Computes a pixels-per-centimeter ratio.</li> <li>Estimates the surface area of sherds/pottery pieces from photos.</li> <li>Provides archaeologists with approximate real-world measurements directly from images.</li> </ul>"},{"location":"apps/#segmentation","title":"\u2702\ufe0f Segmentation","text":"<p>File: <code>segment_streamlit.py</code></p> <ul> <li>Runs segmentation models (RMBG-1.4 + OpenCV) to isolate sherds and measurement cards.</li> <li>Produces clean masks and regions of interest (ROIs) for further analysis.</li> <li>Segments &amp; Classifies color cards, measurement cards in the image.</li> </ul>"},{"location":"apps/#next-steps","title":"Next Steps","text":"<p>These apps serve as experimental prototypes for Phase 1 of the ASCOTA pipeline. In later phases, additional apps will demonstrate:</p> <ul> <li>Classification (Phase 2): automatic labeling by type, color, texture, and decoration/pattern.</li> <li>Full Workflow (Phase 3): an end-to-end UI for archaeologists to process excavation photos.</li> </ul>"},{"location":"api/ascota_classification/","title":"ascota_classification","text":"<p><code>ascota_classification</code> is the second stage of the ASCOTA pipeline. Currently in development.  </p>"},{"location":"api/ascota_classification/#purpose","title":"Purpose","text":"<p>In this package, we focus on:  </p> <ul> <li> <p>Classification of sherd types: Using deep learning models to classify pottery fragments into   predefined categories based on shape and size.  </p> </li> <li> <p>Color Classification: Extracting relevant color features from segmented and color-corrected images     to support classification and analysis.  </p> </li> <li> <p>Texture Classification: Identifying and labeling surface textures of pottery fragments to aid in     classification and comparative analysis.  </p> </li> <li> <p>Pattern/Decoration Classification: Use vision transformer models to identify and classify decorative patterns     on pottery fragments.  </p> </li> </ul>"},{"location":"api/ascota_core/","title":"ascota_core","text":"<p><code>ascota_core</code> is the first stage of the ASCOTA pipeline. It provides the fundamental computer vision and analysis utilities that later modules build upon.</p>"},{"location":"api/ascota_core/#purpose","title":"Purpose","text":"<p>This package contains tools for:</p> <ul> <li> <p>Segmentation &amp; detection   Identifying and isolating key elements in input images (e.g., sherds, measurement cards).   Uses models like SAM (Segment Anything Model) together with OpenCV-based   preprocessing.</p> </li> <li> <p>Color card classification &amp; correction   Detecting reference color cards, classifying patches, and applying   color correction to normalize input images across lighting conditions.</p> </li> <li> <p>Geometric scaling &amp; surface estimation   Leveraging detected measurement cards to compute pixels-per-centimeter   ratios and estimate the surface area of sherds/pottery fragments.</p> </li> </ul>"},{"location":"api/ascota_core/#role-in-the-pipeline","title":"Role in the pipeline","text":"<p><code>ascota_core</code> is the core foundational package that prepares and standardizes input images for downstream classification and analysis. It ensures that: - Sherds are properly segmented from the background. - Color profiles are normalized to a standard reference. - Geometric measurements are calibrated to real-world units (via scale cards).</p> <p>Together, these ensure later classification is performed on calibrated, comparable data.</p>"},{"location":"api/ascota_core/#submodules","title":"Submodules","text":"<ul> <li>color: color correction to standard reference and color based clustering.  </li> <li>imaging: segmentation utilities and color card detection.  </li> <li>scale: scale estimation and pixel-to-centimeter conversion.  </li> <li>utils: shared helpers and lower-level routines.</li> </ul>"},{"location":"api/ascota_core/color/","title":"ascota_core.color","text":"<p>The <code>color</code> module provides functionality for color correction and color-based clustering. It aligns input images to a standard reference using detected color cards, also contains functions to cluster and group similar looking images. This ensures that downstream analysis works with normalized and comparable color values.</p> <p>Color correction and color grading algorithms. Also includes color based clustering algorithms.</p>"},{"location":"api/ascota_core/color/#ascota_core.color.srgb_to_linear","title":"srgb_to_linear","text":"<pre><code>srgb_to_linear(x)\n</code></pre> <p>Convert sRGB color values to linear RGB.</p> <p>Applies the inverse sRGB gamma correction curve to convert from sRGB color space to linear RGB color space for proper color processing calculations.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array of sRGB values in range [0.0, 1.0].</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of linear RGB values in range [0.0, 1.0] with same shape as input.</p> Source code in <code>src\\ascota_core\\color.py</code> <pre><code>def srgb_to_linear(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Convert sRGB color values to linear RGB.\n\n    Applies the inverse sRGB gamma correction curve to convert from sRGB color space\n    to linear RGB color space for proper color processing calculations.\n\n    Args:\n        x: Input array of sRGB values in range [0.0, 1.0].\n\n    Returns:\n        Array of linear RGB values in range [0.0, 1.0] with same shape as input.\n    \"\"\"\n    x = np.clip(x, 0.0, 1.0).astype(np.float32)\n    a = 0.055\n    low = x &lt;= 0.04045\n    high = ~low\n    out = np.empty_like(x, dtype=np.float32)\n    out[low] = x[low] / 12.92\n    out[high] = ((x[high] + a) / (1 + a)) ** 2.4\n    return out\n</code></pre>"},{"location":"api/ascota_core/color/#ascota_core.color.linear_to_srgb","title":"linear_to_srgb","text":"<pre><code>linear_to_srgb(x)\n</code></pre> <p>Convert linear RGB color values to sRGB.</p> <p>Applies the sRGB gamma correction curve to convert from linear RGB color space to sRGB color space for display or output purposes.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input array of linear RGB values in range [0.0, 1.0].</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of sRGB values in range [0.0, 1.0] with same shape as input.</p> Source code in <code>src\\ascota_core\\color.py</code> <pre><code>def linear_to_srgb(x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Convert linear RGB color values to sRGB.\n\n    Applies the sRGB gamma correction curve to convert from linear RGB color space\n    to sRGB color space for display or output purposes.\n\n    Args:\n        x: Input array of linear RGB values in range [0.0, 1.0].\n\n    Returns:\n        Array of sRGB values in range [0.0, 1.0] with same shape as input.\n    \"\"\"\n    x = np.clip(x, 0.0, 1.0).astype(np.float32)\n    a = 0.055\n    low = x &lt;= 0.0031308\n    high = ~low\n    out = np.empty_like(x, dtype=np.float32)\n    out[low] = 12.92 * x[low]\n    out[high] = (1 + a) * (x[high] ** (1 / 2.4)) - a\n    return np.clip(out, 0.0, 1.0)\n</code></pre>"},{"location":"api/ascota_core/color/#ascota_core.color.bgr_linear_to_lab_u8","title":"bgr_linear_to_lab_u8","text":"<pre><code>bgr_linear_to_lab_u8(bgr_lin)\n</code></pre> <p>Convert linear BGR image to 8-bit LAB color space.</p> <p>Converts a linear BGR image to LAB color space for perceptually uniform color operations. The output is in 8-bit format suitable for OpenCV operations.</p> <p>Parameters:</p> Name Type Description Default <code>bgr_lin</code> <code>ndarray</code> <p>Linear BGR image array with values in range [0.0, 1.0].</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>8-bit LAB image array with shape matching input.</p> Source code in <code>src\\ascota_core\\color.py</code> <pre><code>def bgr_linear_to_lab_u8(bgr_lin: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Convert linear BGR image to 8-bit LAB color space.\n\n    Converts a linear BGR image to LAB color space for perceptually uniform\n    color operations. The output is in 8-bit format suitable for OpenCV operations.\n\n    Args:\n        bgr_lin: Linear BGR image array with values in range [0.0, 1.0].\n\n    Returns:\n        8-bit LAB image array with shape matching input.\n    \"\"\"\n    bgr_srgb = linear_to_srgb(np.clip(bgr_lin, 0.0, 1.0))\n    bgr8 = (bgr_srgb * 255.0 + 0.5).astype(np.uint8)\n    lab8 = cv2.cvtColor(bgr8, cv2.COLOR_BGR2LAB)\n    return lab8\n</code></pre>"},{"location":"api/ascota_core/color/#ascota_core.color.lab_u8_to_bgr_linear","title":"lab_u8_to_bgr_linear","text":"<pre><code>lab_u8_to_bgr_linear(lab8)\n</code></pre> <p>Convert 8-bit LAB image to linear BGR color space.</p> <p>Converts a LAB color space image back to linear BGR format for further processing or color correction operations.</p> <p>Parameters:</p> Name Type Description Default <code>lab8</code> <code>ndarray</code> <p>8-bit LAB image array.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Linear BGR image array with values in range [0.0, 1.0].</p> Source code in <code>src\\ascota_core\\color.py</code> <pre><code>def lab_u8_to_bgr_linear(lab8: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Convert 8-bit LAB image to linear BGR color space.\n\n    Converts a LAB color space image back to linear BGR format for further\n    processing or color correction operations.\n\n    Args:\n        lab8: 8-bit LAB image array.\n\n    Returns:\n        Linear BGR image array with values in range [0.0, 1.0].\n    \"\"\"\n    bgr8 = cv2.cvtColor(lab8.astype(np.uint8), cv2.COLOR_LAB2BGR)\n    bgr_srgb = np.clip(bgr8.astype(np.float32) / 255.0, 0.0, 1.0)\n    return srgb_to_linear(bgr_srgb)\n</code></pre>"},{"location":"api/ascota_core/color/#ascota_core.color.to_uint8_preview","title":"to_uint8_preview","text":"<pre><code>to_uint8_preview(bgr_lin)\n</code></pre> <p>Convert linear BGR image to 8-bit RGB PIL Image for preview.</p> <p>Converts a linear BGR image to sRGB color space and creates a PIL Image suitable for display or preview purposes.</p> <p>Parameters:</p> Name Type Description Default <code>bgr_lin</code> <code>ndarray</code> <p>Linear BGR image array with values in range [0.0, 1.0].</p> required <p>Returns:</p> Type Description <code>Image</code> <p>PIL Image in RGB format with 8-bit precision.</p> Source code in <code>src\\ascota_core\\color.py</code> <pre><code>def to_uint8_preview(bgr_lin: np.ndarray) -&gt; Image.Image:\n    \"\"\"Convert linear BGR image to 8-bit RGB PIL Image for preview.\n\n    Converts a linear BGR image to sRGB color space and creates a PIL Image\n    suitable for display or preview purposes.\n\n    Args:\n        bgr_lin: Linear BGR image array with values in range [0.0, 1.0].\n\n    Returns:\n        PIL Image in RGB format with 8-bit precision.\n    \"\"\"\n    bgr_srgb = linear_to_srgb(np.clip(bgr_lin, 0.0, 1.0))\n    rgb8 = (cv2.cvtColor((bgr_srgb * 255.0).astype(np.uint8), cv2.COLOR_BGR2RGB))\n    return Image.fromarray(rgb8)\n</code></pre>"},{"location":"api/ascota_core/color/#ascota_core.color.to_png16_bytes","title":"to_png16_bytes","text":"<pre><code>to_png16_bytes(bgr_lin)\n</code></pre> <p>Convert linear BGR image to 16-bit PNG bytes.</p> <p>Converts a linear BGR image to sRGB color space and encodes it as a 16-bit PNG in bytes format for high-quality output or storage.</p> <p>Parameters:</p> Name Type Description Default <code>bgr_lin</code> <code>ndarray</code> <p>Linear BGR image array with values in range [0.0, 1.0].</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>PNG-encoded bytes of the 16-bit image.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If PNG encoding fails.</p> Source code in <code>src\\ascota_core\\color.py</code> <pre><code>def to_png16_bytes(bgr_lin: np.ndarray) -&gt; bytes:\n    \"\"\"Convert linear BGR image to 16-bit PNG bytes.\n\n    Converts a linear BGR image to sRGB color space and encodes it as a 16-bit\n    PNG in bytes format for high-quality output or storage.\n\n    Args:\n        bgr_lin: Linear BGR image array with values in range [0.0, 1.0].\n\n    Returns:\n        PNG-encoded bytes of the 16-bit image.\n\n    Raises:\n        RuntimeError: If PNG encoding fails.\n    \"\"\"\n    bgr_srgb = linear_to_srgb(np.clip(bgr_lin, 0.0, 1.0))\n    bgr16 = (bgr_srgb * 65535.0 + 0.5).astype(np.uint16)\n    ok, buf = cv2.imencode('.png', bgr16)\n    if not ok:\n        raise RuntimeError(\"Failed to encode PNG16 with OpenCV\")\n    return buf.tobytes()\n</code></pre>"},{"location":"api/ascota_core/color/#ascota_core.color._build_monotone_quantile_mapping","title":"_build_monotone_quantile_mapping","text":"<pre><code>_build_monotone_quantile_mapping(src_vals, tgt_vals, n_knots=257)\n</code></pre> <p>Build monotonic quantile mapping between source and target values.</p> <p>Creates a monotonic mapping from source values to target values using quantiles. This is used for histogram matching and color transfer operations.</p> <p>Parameters:</p> Name Type Description Default <code>src_vals</code> <code>ndarray</code> <p>Source values to map from.</p> required <code>tgt_vals</code> <code>ndarray</code> <p>Target values to map to.</p> required <code>n_knots</code> <code>int</code> <p>Number of quantile knots to use for the mapping. Defaults to 257.</p> <code>257</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Tuple of (source_quantiles, target_quantiles) as float32 arrays for</p> <code>ndarray</code> <p>use with np.interp().</p> Source code in <code>src\\ascota_core\\color.py</code> <pre><code>def _build_monotone_quantile_mapping(src_vals: np.ndarray, tgt_vals: np.ndarray, n_knots: int = 257) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Build monotonic quantile mapping between source and target values.\n\n    Creates a monotonic mapping from source values to target values using\n    quantiles. This is used for histogram matching and color transfer operations.\n\n    Args:\n        src_vals: Source values to map from.\n        tgt_vals: Target values to map to.\n        n_knots: Number of quantile knots to use for the mapping. Defaults to 257.\n\n    Returns:\n        Tuple of (source_quantiles, target_quantiles) as float32 arrays for\n        use with np.interp().\n    \"\"\"\n    q = np.linspace(0.0, 1.0, n_knots, dtype=np.float32)\n    s_q = np.quantile(src_vals.astype(np.float32), q)\n    t_q = np.quantile(tgt_vals.astype(np.float32), q)\n    s_q = np.maximum.accumulate(s_q)\n    eps = 1e-4\n    for i in range(1, len(s_q)):\n        if s_q[i] &lt;= s_q[i-1]:\n            s_q[i] = s_q[i-1] + eps\n    return s_q.astype(np.float32), t_q.astype(np.float32)\n</code></pre>"},{"location":"api/ascota_core/color/#ascota_core.color.apply_manual_adjustments_linear","title":"apply_manual_adjustments_linear","text":"<pre><code>apply_manual_adjustments_linear(\n    bgr_lin, brightness=0, contrast=1.0, gamma=1.0, r_gain=1.0, g_gain=1.0, b_gain=1.0\n)\n</code></pre> <p>Apply manual color adjustments to a linear BGR image.</p> <p>Applies brightness, contrast, gamma, and per-channel gain adjustments to a linear BGR image. All adjustments are performed in sRGB space before converting back to linear.</p> <p>Parameters:</p> Name Type Description Default <code>bgr_lin</code> <code>ndarray</code> <p>Linear BGR image array with values in range [0.0, 1.0].</p> required <code>brightness</code> <code>int</code> <p>Brightness adjustment in range [-255, 255]. Defaults to 0.</p> <code>0</code> <code>contrast</code> <code>float</code> <p>Contrast multiplier, values &gt; 1.0 increase contrast. Defaults to 1.0.</p> <code>1.0</code> <code>gamma</code> <code>float</code> <p>Gamma correction value, values &lt; 1.0 brighten midtones. Defaults to 1.0.</p> <code>1.0</code> <code>r_gain</code> <code>float</code> <p>Red channel gain multiplier. Defaults to 1.0.</p> <code>1.0</code> <code>g_gain</code> <code>float</code> <p>Green channel gain multiplier. Defaults to 1.0.</p> <code>1.0</code> <code>b_gain</code> <code>float</code> <p>Blue channel gain multiplier. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Adjusted linear BGR image array with same shape as input.</p> Source code in <code>src\\ascota_core\\color.py</code> <pre><code>def apply_manual_adjustments_linear(bgr_lin: np.ndarray, brightness: int = 0, contrast: float = 1.0, gamma: float = 1.0,\n                                    r_gain: float = 1.0, g_gain: float = 1.0, b_gain: float = 1.0) -&gt; np.ndarray:\n    \"\"\"Apply manual color adjustments to a linear BGR image.\n\n    Applies brightness, contrast, gamma, and per-channel gain adjustments to\n    a linear BGR image. All adjustments are performed in sRGB space before\n    converting back to linear.\n\n    Args:\n        bgr_lin: Linear BGR image array with values in range [0.0, 1.0].\n        brightness: Brightness adjustment in range [-255, 255]. Defaults to 0.\n        contrast: Contrast multiplier, values &gt; 1.0 increase contrast. Defaults to 1.0.\n        gamma: Gamma correction value, values &lt; 1.0 brighten midtones. Defaults to 1.0.\n        r_gain: Red channel gain multiplier. Defaults to 1.0.\n        g_gain: Green channel gain multiplier. Defaults to 1.0.\n        b_gain: Blue channel gain multiplier. Defaults to 1.0.\n\n    Returns:\n        Adjusted linear BGR image array with same shape as input.\n    \"\"\"\n    bgr_srgb = linear_to_srgb(bgr_lin)\n    gains = np.array([b_gain, g_gain, r_gain], dtype=np.float32).reshape(1, 1, 3)\n    x = np.clip(bgr_srgb * contrast + (brightness / 255.0), 0.0, 1.0)\n    x = np.clip(x * gains, 0.0, 1.0)\n    x = np.power(np.clip(x, 1e-6, 1.0), 1.0 / max(gamma, 1e-6))\n    return srgb_to_linear(np.clip(x, 0.0, 1.0))\n</code></pre>"},{"location":"api/ascota_core/color/#ascota_core.color.lab_mean_std_transfer_linear","title":"lab_mean_std_transfer_linear","text":"<pre><code>lab_mean_std_transfer_linear(src_bgr_lin, tgt_bgr_lin, src_mask, tgt_mask)\n</code></pre> <p>Transfer color statistics using LAB mean and standard deviation.</p> <p>Performs color transfer by matching the mean and standard deviation of each LAB channel within the masked regions. This method preserves luminance relationships while adjusting color characteristics.</p> <p>Parameters:</p> Name Type Description Default <code>src_bgr_lin</code> <code>ndarray</code> <p>Source linear BGR image array.</p> required <code>tgt_bgr_lin</code> <code>ndarray</code> <p>Target linear BGR image array.</p> required <code>src_mask</code> <code>ndarray</code> <p>Boolean mask for source image region of interest.</p> required <code>tgt_mask</code> <code>ndarray</code> <p>Boolean mask for target image region of interest.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Color-corrected linear BGR image array with same shape as source.</p> Source code in <code>src\\ascota_core\\color.py</code> <pre><code>def lab_mean_std_transfer_linear(src_bgr_lin: np.ndarray, tgt_bgr_lin: np.ndarray,\n                                 src_mask: np.ndarray, tgt_mask: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Transfer color statistics using LAB mean and standard deviation.\n\n    Performs color transfer by matching the mean and standard deviation\n    of each LAB channel within the masked regions. This method preserves\n    luminance relationships while adjusting color characteristics.\n\n    Args:\n        src_bgr_lin: Source linear BGR image array.\n        tgt_bgr_lin: Target linear BGR image array.\n        src_mask: Boolean mask for source image region of interest.\n        tgt_mask: Boolean mask for target image region of interest.\n\n    Returns:\n        Color-corrected linear BGR image array with same shape as source.\n    \"\"\"\n    src_lab = bgr_linear_to_lab_u8(src_bgr_lin)\n    tgt_lab = bgr_linear_to_lab_u8(tgt_bgr_lin)\n    out = src_lab.astype(np.float32)\n    for c in range(3):\n        s_vals = src_lab[..., c][src_mask].astype(np.float32)\n        t_vals = tgt_lab[..., c][tgt_mask].astype(np.float32)\n        if len(s_vals) == 0 or len(t_vals) == 0:\n            continue\n        s_mean, s_std = float(np.mean(s_vals)), float(np.std(s_vals) + 1e-6)\n        t_mean, t_std = float(np.mean(t_vals)), float(np.std(t_vals))\n        out[..., c] = (out[..., c] - s_mean) * (t_std / s_std) + t_mean\n    out_u8 = np.clip(out, 0.0, 255.0).astype(np.uint8)\n    return lab_u8_to_bgr_linear(out_u8)\n</code></pre>"},{"location":"api/ascota_core/color/#ascota_core.color.monotone_lut_ab_only","title":"monotone_lut_ab_only","text":"<pre><code>monotone_lut_ab_only(src_bgr_lin, tgt_bgr_lin, src_mask, tgt_mask, n_knots=257)\n</code></pre> <p>Apply monotonic LUT mapping to A and B channels only in LAB space.</p> <p>Performs color transfer by applying quantile-based monotonic lookup tables to the A and B channels in LAB color space, while preserving the L channel. This method focuses on chromaticity transfer without affecting luminance.</p> <p>Parameters:</p> Name Type Description Default <code>src_bgr_lin</code> <code>ndarray</code> <p>Source linear BGR image array.</p> required <code>tgt_bgr_lin</code> <code>ndarray</code> <p>Target linear BGR image array.</p> required <code>src_mask</code> <code>ndarray</code> <p>Boolean mask for source image region of interest.</p> required <code>tgt_mask</code> <code>ndarray</code> <p>Boolean mask for target image region of interest.</p> required <code>n_knots</code> <code>int</code> <p>Number of quantile knots for the LUT mapping. Defaults to 257.</p> <code>257</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Color-corrected linear BGR image array with same shape as source.</p> Source code in <code>src\\ascota_core\\color.py</code> <pre><code>def monotone_lut_ab_only(src_bgr_lin: np.ndarray, tgt_bgr_lin: np.ndarray,\n                         src_mask: np.ndarray, tgt_mask: np.ndarray,\n                         n_knots: int = 257) -&gt; np.ndarray:\n    \"\"\"Apply monotonic LUT mapping to A and B channels only in LAB space.\n\n    Performs color transfer by applying quantile-based monotonic lookup tables\n    to the A and B channels in LAB color space, while preserving the L channel.\n    This method focuses on chromaticity transfer without affecting luminance.\n\n    Args:\n        src_bgr_lin: Source linear BGR image array.\n        tgt_bgr_lin: Target linear BGR image array.\n        src_mask: Boolean mask for source image region of interest.\n        tgt_mask: Boolean mask for target image region of interest.\n        n_knots: Number of quantile knots for the LUT mapping. Defaults to 257.\n\n    Returns:\n        Color-corrected linear BGR image array with same shape as source.\n    \"\"\"\n    src_lab = bgr_linear_to_lab_u8(src_bgr_lin)\n    tgt_lab = bgr_linear_to_lab_u8(tgt_bgr_lin)\n    out = src_lab.astype(np.float32)\n    for c in [1, 2]:\n        s_vals = src_lab[..., c][src_mask].astype(np.float32)\n        t_vals = tgt_lab[..., c][tgt_mask].astype(np.float32)\n        if len(s_vals) == 0 or len(t_vals) == 0:\n            continue\n        xk, yk = _build_monotone_quantile_mapping(s_vals, t_vals, n_knots=n_knots)\n        ch = out[..., c]\n        out[..., c] = np.interp(ch, xk, yk).astype(np.float32)\n    out_u8 = np.clip(out, 0.0, 255.0).astype(np.uint8)\n    return lab_u8_to_bgr_linear(out_u8)\n</code></pre>"},{"location":"api/ascota_core/color/#ascota_core.color.masked_histogram_match_rgb_linear","title":"masked_histogram_match_rgb_linear","text":"<pre><code>masked_histogram_match_rgb_linear(src_bgr_lin, tgt_bgr_lin, src_mask, tgt_mask, n_knots=1025)\n</code></pre> <p>Perform histogram matching in RGB space using masked regions.</p> <p>Applies quantile-based histogram matching to each RGB channel independently using only the pixels within the specified mask regions. This method provides comprehensive color transfer across all channels.</p> <p>Parameters:</p> Name Type Description Default <code>src_bgr_lin</code> <code>ndarray</code> <p>Source linear BGR image array.</p> required <code>tgt_bgr_lin</code> <code>ndarray</code> <p>Target linear BGR image array.</p> required <code>src_mask</code> <code>ndarray</code> <p>Boolean mask for source image region of interest.</p> required <code>tgt_mask</code> <code>ndarray</code> <p>Boolean mask for target image region of interest.</p> required <code>n_knots</code> <code>int</code> <p>Number of quantile knots for histogram matching. Defaults to 1025.</p> <code>1025</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Color-corrected linear BGR image array with same shape as source.</p> Source code in <code>src\\ascota_core\\color.py</code> <pre><code>def masked_histogram_match_rgb_linear(src_bgr_lin: np.ndarray, tgt_bgr_lin: np.ndarray,\n                                      src_mask: np.ndarray, tgt_mask: np.ndarray,\n                                      n_knots: int = 1025) -&gt; np.ndarray:\n    \"\"\"Perform histogram matching in RGB space using masked regions.\n\n    Applies quantile-based histogram matching to each RGB channel independently\n    using only the pixels within the specified mask regions. This method provides\n    comprehensive color transfer across all channels.\n\n    Args:\n        src_bgr_lin: Source linear BGR image array.\n        tgt_bgr_lin: Target linear BGR image array.\n        src_mask: Boolean mask for source image region of interest.\n        tgt_mask: Boolean mask for target image region of interest.\n        n_knots: Number of quantile knots for histogram matching. Defaults to 1025.\n\n    Returns:\n        Color-corrected linear BGR image array with same shape as source.\n    \"\"\"\n    src_srgb = linear_to_srgb(src_bgr_lin)\n    tgt_srgb = linear_to_srgb(tgt_bgr_lin)\n    src_rgb = cv2.cvtColor(src_srgb, cv2.COLOR_BGR2RGB)\n    tgt_rgb = cv2.cvtColor(tgt_srgb, cv2.COLOR_BGR2RGB)\n    out_rgb = src_rgb.copy()\n    for c in range(3):\n        s_vals = src_rgb[..., c][src_mask].astype(np.float32)\n        t_vals = tgt_rgb[..., c][tgt_mask].astype(np.float32)\n        if len(s_vals) == 0 or len(t_vals) == 0:\n            continue\n        s_vals255 = s_vals * 255.0\n        t_vals255 = t_vals * 255.0\n        xk, yk = _build_monotone_quantile_mapping(s_vals255, t_vals255, n_knots=n_knots)\n        ch = (out_rgb[..., c] * 255.0).astype(np.float32)\n        mapped = np.interp(ch, xk, yk) / 255.0\n        out_rgb[..., c] = np.clip(mapped, 0.0, 1.0)\n    out_bgr_srgb = cv2.cvtColor(out_rgb, cv2.COLOR_RGB2BGR)\n    return srgb_to_linear(out_bgr_srgb)\n</code></pre>"},{"location":"api/ascota_core/color/#ascota_core.color._pil_to_bgr_linear","title":"_pil_to_bgr_linear","text":"<pre><code>_pil_to_bgr_linear(img_pil)\n</code></pre> <p>Convert PIL Image to linear BGR array.</p> <p>Converts a PIL Image to linear BGR format suitable for color processing operations. The conversion goes through RGB -&gt; sRGB -&gt; linear RGB -&gt; BGR.</p> <p>Parameters:</p> Name Type Description Default <code>img_pil</code> <code>Image</code> <p>Input PIL Image in any supported format.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Linear BGR image array with values in range [0.0, 1.0].</p> Source code in <code>src\\ascota_core\\color.py</code> <pre><code>def _pil_to_bgr_linear(img_pil: Image.Image) -&gt; np.ndarray:\n    \"\"\"Convert PIL Image to linear BGR array.\n\n    Converts a PIL Image to linear BGR format suitable for color processing\n    operations. The conversion goes through RGB -&gt; sRGB -&gt; linear RGB -&gt; BGR.\n\n    Args:\n        img_pil: Input PIL Image in any supported format.\n\n    Returns:\n        Linear BGR image array with values in range [0.0, 1.0].\n    \"\"\"\n    rgb8 = np.array(img_pil.convert(\"RGB\"), dtype=np.uint8)\n    rgb_srgb = rgb8.astype(np.float32) / 255.0\n    rgb_lin = srgb_to_linear(rgb_srgb)\n    bgr_lin = cv2.cvtColor(rgb_lin, cv2.COLOR_RGB2BGR)\n    return bgr_lin\n</code></pre>"},{"location":"api/ascota_core/color/#ascota_core.color._prepare_mask_bool","title":"_prepare_mask_bool","text":"<pre><code>_prepare_mask_bool(mask_pil, expected_shape)\n</code></pre> <p>Convert PIL mask to boolean array aligned to expected dimensions.</p> <p>Converts a PIL mask image to a boolean array where True indicates the region of interest (card area). Resizes the mask if needed to match the expected dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>mask_pil</code> <code>Image</code> <p>PIL mask image, will be converted to grayscale.</p> required <code>expected_shape</code> <code>Tuple[int, int]</code> <p>Target dimensions as (width, height) tuple.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Boolean mask array with shape (height, width) where True indicates</p> <code>ndarray</code> <p>the card region (pixels with value &gt;= 200).</p> Source code in <code>src\\ascota_core\\color.py</code> <pre><code>def _prepare_mask_bool(mask_pil: Image.Image, expected_shape: Tuple[int, int]) -&gt; np.ndarray:\n    \"\"\"Convert PIL mask to boolean array aligned to expected dimensions.\n\n    Converts a PIL mask image to a boolean array where True indicates the\n    region of interest (card area). Resizes the mask if needed to match\n    the expected dimensions.\n\n    Args:\n        mask_pil: PIL mask image, will be converted to grayscale.\n        expected_shape: Target dimensions as (width, height) tuple.\n\n    Returns:\n        Boolean mask array with shape (height, width) where True indicates\n        the card region (pixels with value &gt;= 200).\n    \"\"\"\n    mask = mask_pil.convert(\"L\")\n    arr = np.array(mask, dtype=np.uint8)\n    # expected_shape is (width, height) to match how read_mask used in streamlit; convert\n    expected_w, expected_h = expected_shape\n    if (arr.shape[1], arr.shape[0]) != (expected_w, expected_h):\n        arr = cv2.resize(arr, (expected_w, expected_h), interpolation=cv2.INTER_NEAREST)\n    return arr &gt;= 200\n</code></pre>"},{"location":"api/ascota_core/color/#ascota_core.color.match_color_cards_from_pils","title":"match_color_cards_from_pils","text":"<pre><code>match_color_cards_from_pils(\n    original_image,\n    original_masks,\n    target_image,\n    target_masks,\n    original_card_types,\n    target_card_types,\n    method=\"lab_mean_std_transfer\",\n    n_knots=None,\n    debug=False,\n)\n</code></pre> <p>Match color appearance between two images using color card references.</p> <p>Performs color correction by matching the appearance of color cards between a source image and target image. The function automatically finds matching color card types and applies the specified color transfer method.</p> <p>Parameters:</p> Name Type Description Default <code>original_image</code> <code>Image</code> <p>Source PIL Image to correct.</p> required <code>original_masks</code> <code>List[Image]</code> <p>List of PIL mask images for detected cards in source image.</p> required <code>target_image</code> <code>Image</code> <p>Target PIL Image providing the desired color appearance.</p> required <code>target_masks</code> <code>List[Image]</code> <p>List of PIL mask images for detected cards in target image.</p> required <code>original_card_types</code> <code>List[str]</code> <p>List of card type strings corresponding to original_masks.</p> required <code>target_card_types</code> <code>List[str]</code> <p>List of card type strings corresponding to target_masks.</p> required <code>method</code> <code>str</code> <p>Color transfer method to use. One of: - \"lab_mean_std_transfer\" (default): Mean/std matching in LAB space - \"monotone_lut\": Quantile mapping on LAB A/B channels only - \"histogram_matching\": Full RGB histogram matching</p> <code>'lab_mean_std_transfer'</code> <code>n_knots</code> <code>Optional[int]</code> <p>Number of quantile knots for LUT-based methods. If None, uses method-specific defaults (513 for monotone_lut, 1025 for histogram_matching).</p> <code>None</code> <code>debug</code> <code>bool</code> <p>Enable debug output. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing: - matched_preview: 8-bit PIL Image preview of corrected result - matched_png16: 16-bit PNG encoded bytes of corrected result - used_card_type: Name of color card type used ('colorchecker24' or 'colorchecker8') - method: The color transfer method that was applied</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no matching color card types are found in both images, or if an unknown method is specified.</p> Source code in <code>src\\ascota_core\\color.py</code> <pre><code>def match_color_cards_from_pils(\n    original_image: Image.Image,\n    original_masks: List[Image.Image],\n    target_image: Image.Image, \n    target_masks: List[Image.Image],\n    original_card_types: List[str],\n    target_card_types: List[str],\n    method: str = \"lab_mean_std_transfer\",\n    n_knots: Optional[int] = None,\n    debug: bool = False,\n) -&gt; Dict[str, Any]:\n    \"\"\"Match color appearance between two images using color card references.\n\n    Performs color correction by matching the appearance of color cards between\n    a source image and target image. The function automatically finds matching\n    color card types and applies the specified color transfer method.\n\n    Args:\n        original_image: Source PIL Image to correct.\n        original_masks: List of PIL mask images for detected cards in source image.\n        target_image: Target PIL Image providing the desired color appearance.\n        target_masks: List of PIL mask images for detected cards in target image.\n        original_card_types: List of card type strings corresponding to original_masks.\n        target_card_types: List of card type strings corresponding to target_masks.\n        method: Color transfer method to use. One of:\n            - \"lab_mean_std_transfer\" (default): Mean/std matching in LAB space\n            - \"monotone_lut\": Quantile mapping on LAB A/B channels only\n            - \"histogram_matching\": Full RGB histogram matching\n        n_knots: Number of quantile knots for LUT-based methods. If None, uses\n            method-specific defaults (513 for monotone_lut, 1025 for histogram_matching).\n        debug: Enable debug output. Defaults to False.\n\n    Returns:\n        Dictionary containing:\n            - matched_preview: 8-bit PIL Image preview of corrected result\n            - matched_png16: 16-bit PNG encoded bytes of corrected result  \n            - used_card_type: Name of color card type used ('colorchecker24' or 'colorchecker8')\n            - method: The color transfer method that was applied\n\n    Raises:\n        ValueError: If no matching color card types are found in both images,\n            or if an unknown method is specified.\n    \"\"\"\n\n    # select preferred card types\n    preferred = ['colorchecker24', 'colorchecker8']\n\n    # Helper to find indices by type list\n    def _index_of_type(types_list: List[str], masks_list: List[Image.Image], desired: str) -&gt; Optional[int]:\n        for i, t in enumerate(types_list):\n            if t == desired and i &lt; len(masks_list):\n                return i\n        return None\n\n    # Find a matching preferred type present in both sets\n    chosen_type = None\n    orig_idx = tgt_idx = None\n    for t in preferred:\n        o_i = _index_of_type(original_card_types, original_masks, t)\n        ti = _index_of_type(target_card_types, target_masks, t)\n        if o_i is not None and ti is not None:\n            chosen_type = t\n            orig_idx = o_i\n            tgt_idx = ti\n            break\n\n    if chosen_type is None:\n        raise ValueError(\"Could not find matching colorchecker24/colorchecker8 masks in both inputs\")\n\n    # Convert PIL images to linear BGR arrays\n    src_lin = _pil_to_bgr_linear(original_image)\n    tgt_lin = _pil_to_bgr_linear(target_image)\n\n    # Prepare boolean masks aligned to images\n    src_mask_bool = _prepare_mask_bool(original_masks[orig_idx], (src_lin.shape[1], src_lin.shape[0]))\n    tgt_mask_bool = _prepare_mask_bool(target_masks[tgt_idx], (tgt_lin.shape[1], tgt_lin.shape[0]))\n\n    # Choose algorithm\n    algo_map = {\n        \"lab_mean_std_transfer\": lambda s, t, sm, tm: lab_mean_std_transfer_linear(s, t, sm, tm),\n        \"monotone_lut\": lambda s, t, sm, tm: monotone_lut_ab_only(s, t, sm, tm, n_knots=(n_knots or 513)),\n        \"histogram_matching\": lambda s, t, sm, tm: masked_histogram_match_rgb_linear(s, t, sm, tm, n_knots=(n_knots or 1025)),\n    }\n\n    if method not in algo_map:\n        raise ValueError(f\"Unknown method '{method}'. Valid: {list(algo_map.keys())}\")\n\n    matched_lin = algo_map[method](src_lin, tgt_lin, src_mask_bool, tgt_mask_bool)\n\n    # Prepare outputs\n    matched_preview = to_uint8_preview(matched_lin)\n    matched_png16 = to_png16_bytes(matched_lin)\n\n    return {\n        'matched_preview': matched_preview,\n        'matched_png16': matched_png16,\n        'used_card_type': chosen_type,\n        'method': method\n    }\n</code></pre>"},{"location":"api/ascota_core/color/#ascota_core.color.match_color_cards_from_pipeline_outputs","title":"match_color_cards_from_pipeline_outputs","text":"<pre><code>match_color_cards_from_pipeline_outputs(\n    original_pipeline_output,\n    target_pipeline_output,\n    method=\"lab_mean_std_transfer\",\n    n_knots=None,\n    debug=False,\n)\n</code></pre> <p>Match color cards using outputs from the image processing pipeline.</p> <p>Convenience wrapper that accepts dictionaries returned by process_image_pipeline() and performs color matching between source and target images using their detected color cards.</p> <p>Parameters:</p> Name Type Description Default <code>original_pipeline_output</code> <code>dict</code> <p>Dictionary from process_image_pipeline() for source image. Must contain keys: 'original_image', 'masks', 'card_types'.</p> required <code>target_pipeline_output</code> <code>dict</code> <p>Dictionary from process_image_pipeline() for target image. Must contain keys: 'original_image', 'masks', 'card_types'.</p> required <code>method</code> <code>str</code> <p>Color transfer method to use. See match_color_cards_from_pils() for options. Defaults to \"lab_mean_std_transfer\".</p> <code>'lab_mean_std_transfer'</code> <code>n_knots</code> <code>Optional[int]</code> <p>Number of quantile knots for LUT-based methods. If None, uses method-specific defaults.</p> <code>None</code> <code>debug</code> <code>bool</code> <p>Enable debug output. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with same structure as match_color_cards_from_pils().</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If pipeline outputs are not dictionaries.</p> <code>ValueError</code> <p>If required keys are missing from pipeline outputs.</p> Source code in <code>src\\ascota_core\\color.py</code> <pre><code>def match_color_cards_from_pipeline_outputs(\n    original_pipeline_output: dict,\n    target_pipeline_output: dict,\n    method: str = \"lab_mean_std_transfer\",\n    n_knots: Optional[int] = None,\n    debug: bool = False,\n) -&gt; Dict[str, Any]:\n    \"\"\"Match color cards using outputs from the image processing pipeline.\n\n    Convenience wrapper that accepts dictionaries returned by process_image_pipeline()\n    and performs color matching between source and target images using their\n    detected color cards.\n\n    Args:\n        original_pipeline_output: Dictionary from process_image_pipeline() for source image.\n            Must contain keys: 'original_image', 'masks', 'card_types'.\n        target_pipeline_output: Dictionary from process_image_pipeline() for target image.\n            Must contain keys: 'original_image', 'masks', 'card_types'.\n        method: Color transfer method to use. See match_color_cards_from_pils() for options.\n            Defaults to \"lab_mean_std_transfer\".\n        n_knots: Number of quantile knots for LUT-based methods. If None, uses\n            method-specific defaults.\n        debug: Enable debug output. Defaults to False.\n\n    Returns:\n        Dictionary with same structure as match_color_cards_from_pils().\n\n    Raises:\n        TypeError: If pipeline outputs are not dictionaries.\n        ValueError: If required keys are missing from pipeline outputs.\n    \"\"\"\n    # Basic validation / helpful error messages\n    for name, out in ((\"original_pipeline_output\", original_pipeline_output), (\"target_pipeline_output\", target_pipeline_output)):\n        if not isinstance(out, dict):\n            raise TypeError(f\"{name} must be a dict as returned by process_image_pipeline()\")\n\n    try:\n        orig_img = original_pipeline_output['original_image']\n        orig_masks = original_pipeline_output['masks']\n    except KeyError as e:\n        raise ValueError(\"original_pipeline_output missing required key 'original_image' or 'masks'\") from e\n\n    try:\n        tgt_img = target_pipeline_output['original_image']\n        tgt_masks = target_pipeline_output['masks']\n    except KeyError as e:\n        raise ValueError(\"target_pipeline_output missing required key 'original_image' or 'masks'\") from e\n\n    orig_types = original_pipeline_output.get('card_types')\n    tgt_types = target_pipeline_output.get('card_types')\n\n    return match_color_cards_from_pils(\n        original_image=orig_img,\n        original_masks=orig_masks,\n        target_image=tgt_img,\n        target_masks=tgt_masks,\n        original_card_types=orig_types,\n        target_card_types=tgt_types,\n        method=method,\n        n_knots=n_knots,\n        debug=debug,\n    )\n</code></pre>"},{"location":"api/ascota_core/imaging/","title":"ascota_core.imaging","text":"<p>The <code>imaging</code> module implements segmentation utilities and color card detection. It leverages different models and traditional computer vision techniques (OpenCV) to isolate pottery sherds, measurement cards, and other regions of interest. We also use template matching to enhance detection accuracy. This module is crucial for preparing images for subsequent color correction  and scale estimation.</p> <p>Segmentation, Color card detection and classification pipeline.</p>"},{"location":"api/ascota_core/imaging/#ascota_core.imaging.TemplateMatcher","title":"TemplateMatcher","text":"<pre><code>TemplateMatcher(template_bgr, name, nfeatures=1500)\n</code></pre> <p>Feature-based template matcher for color card detection using ORB features.</p> <p>Initialize template matcher with ORB feature detection.</p> <p>Creates a template matcher that uses ORB (Oriented FAST and Rotated BRIEF) features for robust detection of color cards in images. The template is preprocessed to extract keypoints and descriptors for matching.</p> <p>Parameters:</p> Name Type Description Default <code>template_bgr</code> <code>ndarray</code> <p>Template image in OpenCV BGR format as numpy array.</p> required <code>name</code> <code>str</code> <p>Identifier name for this template (e.g., 'colorchecker24').</p> required <code>nfeatures</code> <code>int</code> <p>Maximum number of ORB features to detect. Defaults to 1500.</p> <code>1500</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If template_bgr is None, empty, or cannot be converted to grayscale.</p> <code>TypeError</code> <p>If template_bgr is not a numpy array.</p> Source code in <code>src\\ascota_core\\imaging.py</code> <pre><code>def __init__(self, template_bgr: np.ndarray, name: str, nfeatures: int = 1500) -&gt; None:\n    \"\"\"Initialize template matcher with ORB feature detection.\n\n    Creates a template matcher that uses ORB (Oriented FAST and Rotated BRIEF)\n    features for robust detection of color cards in images. The template is\n    preprocessed to extract keypoints and descriptors for matching.\n\n    Args:\n        template_bgr: Template image in OpenCV BGR format as numpy array.\n        name: Identifier name for this template (e.g., 'colorchecker24').\n        nfeatures: Maximum number of ORB features to detect. Defaults to 1500.\n\n    Raises:\n        ValueError: If template_bgr is None, empty, or cannot be converted to grayscale.\n        TypeError: If template_bgr is not a numpy array.\n    \"\"\"\n    self.name = name\n    # Validate template image\n    if template_bgr is None:\n        raise ValueError(f\"Template image for '{name}' is None\")\n    if not isinstance(template_bgr, (np.ndarray,)):\n        raise TypeError(f\"Template image for '{name}' must be a numpy.ndarray, got {type(template_bgr)}\")\n    if template_bgr.size == 0:\n        raise ValueError(f\"Template image for '{name}' is empty\")\n\n    self.tmpl = template_bgr\n    try:\n        self.gray = cv2.cvtColor(template_bgr, cv2.COLOR_BGR2GRAY)\n    except Exception as e:\n        raise ValueError(f\"Failed to convert template '{name}' to grayscale: {e}\")\n    self.h, self.w = self.gray.shape\n    self.corners = np.float32([[0,0],[self.w,0],[self.w,self.h],[0,self.h]]).reshape(-1,1,2)\n\n    self.orb = cv2.ORB_create(nfeatures=nfeatures, scaleFactor=1.2, nlevels=8, edgeThreshold=15)\n    self.kp, self.des = self.orb.detectAndCompute(self.gray, None)\n\n    self.bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=False)\n</code></pre>"},{"location":"api/ascota_core/imaging/#ascota_core.imaging.TemplateMatcher.find_instances","title":"find_instances","text":"<pre><code>find_instances(img_bgr, max_instances=5, debug=False)\n</code></pre> <p>Find instances of the template in the input image using feature matching.</p> <p>Uses ORB feature matching with homography estimation to detect instances of the template card in the input image. Applies geometric validation including convexity and area checks to ensure quality detections.</p> <p>Parameters:</p> Name Type Description Default <code>img_bgr</code> <code>ndarray</code> <p>Input image in OpenCV BGR format as numpy array.</p> required <code>max_instances</code> <code>int</code> <p>Maximum number of template instances to detect. Defaults to 5.</p> <code>5</code> <code>debug</code> <code>bool</code> <p>If True, print similarity scores and detection information.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[ndarray]</code> <p>List of detected polygon coordinates as numpy arrays with shape (4, 2).</p> <code>List[ndarray]</code> <p>Each polygon represents the four corners of a detected card instance.</p> <code>List[ndarray]</code> <p>Results are filtered using non-maximum suppression to remove overlaps.</p> Source code in <code>src\\ascota_core\\imaging.py</code> <pre><code>def find_instances(self, img_bgr: np.ndarray, max_instances: int = 5, debug: bool = False) -&gt; List[np.ndarray]:\n    \"\"\"Find instances of the template in the input image using feature matching.\n\n    Uses ORB feature matching with homography estimation to detect instances\n    of the template card in the input image. Applies geometric validation\n    including convexity and area checks to ensure quality detections.\n\n    Args:\n        img_bgr: Input image in OpenCV BGR format as numpy array.\n        max_instances: Maximum number of template instances to detect. Defaults to 5.\n        debug: If True, print similarity scores and detection information.\n\n    Returns:\n        List of detected polygon coordinates as numpy arrays with shape (4, 2).\n        Each polygon represents the four corners of a detected card instance.\n        Results are filtered using non-maximum suppression to remove overlaps.\n    \"\"\"\n    found_polys = []\n    img = img_bgr.copy()\n    for _ in range(max_instances):\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        kp2, des2 = self.orb.detectAndCompute(gray, None)\n        if des2 is None or len(kp2) &lt; 20:\n            break\n\n        matches = self.bf.knnMatch(self.des, des2, k=2)\n        good = []\n        for m, n in matches:\n            if m.distance &lt; 0.75 * n.distance:\n                good.append(m)\n\n        if len(good) &lt; 12:\n            break\n\n        src_pts = np.float32([ self.kp[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n        dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n\n        H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, ransacReprojThreshold=4.0)\n        if H is None:\n            break\n\n        # project template corners\n        proj = cv2.perspectiveTransform(self.corners, H).reshape(4,2)\n        # sanity checks: convex, area, aspect\n        area = cv2.contourArea(proj.astype(np.float32))\n        if area &lt; 400:  # too small\n            break\n        if not cv2.isContourConvex(proj.astype(np.float32)):\n            break\n\n        # Calculate similarity for debugging\n        if debug:\n            try:\n                similarity = self._calculate_similarity(img_bgr, proj, H)\n                area = cv2.contourArea(proj.astype(np.float32))\n                print(f\"DEBUG: TemplateMatcher '{self.name}' - Detected card similarity: {similarity:.2f}% (area: {area:.0f})\")\n            except ValueError as e:\n                print(f\"DEBUG: TemplateMatcher '{self.name}' - {e}\")\n                continue  # Skip this detection due to low similarity\n\n        # accept\n        found_polys.append(proj)\n\n        # \"erase\" region to find more instances\n        mask_poly = polygon_to_mask(img.shape, proj)\n        img[mask_poly&gt;0] = 255  # fill with white\n\n    # suppress overlaps (different templates might hit same card)\n    found_polys = non_max_suppression_polys(found_polys, iou_thresh=0.3)\n    return found_polys\n</code></pre>"},{"location":"api/ascota_core/imaging/#ascota_core.imaging.TemplateMatcher._calculate_similarity","title":"_calculate_similarity","text":"<pre><code>_calculate_similarity(img_bgr, detected_poly, homography, min_similarity_threshold=30.0)\n</code></pre> <p>Calculate similarity percentage between detected card region and template.</p> <p>Warps the detected card region to match template dimensions and computes a similarity score based on structural similarity and feature matching. Used for quality assessment of card detections.</p> <p>Parameters:</p> Name Type Description Default <code>img_bgr</code> <code>ndarray</code> <p>Input image in BGR format containing the detected card.</p> required <code>detected_poly</code> <code>ndarray</code> <p>Detected card polygon with shape (4, 2) representing corners.</p> required <code>homography</code> <code>ndarray</code> <p>3x3 homography matrix from template to detected region.</p> required <code>min_similarity_threshold</code> <code>float</code> <p>Minimum similarity score required. Defaults to 30.0.</p> <code>30.0</code> <p>Returns:</p> Type Description <code>float</code> <p>Similarity percentage as float between 0.0 and 100.0.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If similarity is below the minimum threshold.</p> <p>Parameters:</p> Name Type Description Default <code>img_bgr</code> <code>ndarray</code> <p>Input image in BGR format</p> required <code>detected_poly</code> <code>ndarray</code> <p>Detected polygon coordinates</p> required <code>homography</code> <code>ndarray</code> <p>Homography matrix from template to detected region</p> required <code>min_similarity_threshold</code> <code>float</code> <p>Minimum similarity threshold (0-100), raises exception if below</p> <code>30.0</code> <p>Returns:</p> Type Description <code>float</code> <p>Similarity percentage (0-100)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If similarity is below the minimum threshold</p> Source code in <code>src\\ascota_core\\imaging.py</code> <pre><code>def _calculate_similarity(self, img_bgr: np.ndarray, detected_poly: np.ndarray, \n                         homography: np.ndarray, min_similarity_threshold: float = 30.0) -&gt; float:\n    \"\"\"Calculate similarity percentage between detected card region and template.\n\n    Warps the detected card region to match template dimensions and computes\n    a similarity score based on structural similarity and feature matching.\n    Used for quality assessment of card detections.\n\n    Args:\n        img_bgr: Input image in BGR format containing the detected card.\n        detected_poly: Detected card polygon with shape (4, 2) representing corners.\n        homography: 3x3 homography matrix from template to detected region.\n        min_similarity_threshold: Minimum similarity score required. Defaults to 30.0.\n\n    Returns:\n        Similarity percentage as float between 0.0 and 100.0.\n\n    Raises:\n        ValueError: If similarity is below the minimum threshold.\n\n    Args:\n        img_bgr: Input image in BGR format\n        detected_poly: Detected polygon coordinates\n        homography: Homography matrix from template to detected region\n        min_similarity_threshold: Minimum similarity threshold (0-100), raises exception if below\n\n    Returns:\n        Similarity percentage (0-100)\n\n    Raises:\n        ValueError: If similarity is below the minimum threshold\n    \"\"\"\n    try:\n        # Extract the detected region from the input image\n        x, y, w, h = cv2.boundingRect(detected_poly.astype(np.int32))\n\n        # Ensure coordinates are within image bounds\n        img_h, img_w = img_bgr.shape[:2]\n        x = max(0, min(x, img_w - 1))\n        y = max(0, min(y, img_h - 1))\n        w = min(w, img_w - x)\n        h = min(h, img_h - y)\n\n        # Check if region is valid\n        if w &lt;= 0 or h &lt;= 0:\n            raise ValueError(f\"Invalid detected region: x={x}, y={y}, w={w}, h={h}\")\n\n        detected_region = cv2.cvtColor(img_bgr[y:y+h, x:x+w], cv2.COLOR_BGR2GRAY)\n\n        # Check if detected region is valid\n        if detected_region.size == 0:\n            raise ValueError(\"Detected region is empty\")\n\n        # Resize template to match detected region size for comparison\n        template_resized = cv2.resize(self.gray, (w, h))\n\n        # Calculate structural similarity using template matching\n        result = cv2.matchTemplate(detected_region, template_resized, cv2.TM_CCOEFF_NORMED)\n        similarity = float(result[0][0]) * 100\n\n        # Additional check using histogram correlation for color similarity\n        hist_template = cv2.calcHist([template_resized], [0], None, [256], [0, 256])\n        hist_detected = cv2.calcHist([detected_region], [0], None, [256], [0, 256])\n        hist_corr = cv2.compareHist(hist_template, hist_detected, cv2.HISTCMP_CORREL)\n\n        # Combine template matching and histogram correlation\n        combined_similarity = (similarity + hist_corr * 100) / 2\n        final_similarity = max(0, min(100, combined_similarity))\n\n        # Check if similarity is too low\n        if final_similarity &lt; min_similarity_threshold:\n            raise ValueError(f\"Low similarity detected: {final_similarity:.2f}% (threshold: {min_similarity_threshold}%) for template '{self.name}'. This may indicate a false positive detection.\")\n\n        return final_similarity\n\n    except ValueError:\n        # Re-raise ValueError for low similarity\n        raise\n    except Exception as e:\n        print(f\"DEBUG: _calculate_similarity failed: {e}\")\n        raise ValueError(f\"Similarity calculation failed for template '{self.name}': {e}\")\n</code></pre>"},{"location":"api/ascota_core/imaging/#ascota_core.imaging.CardDetector","title":"CardDetector","text":"<pre><code>CardDetector(template_paths)\n</code></pre> <p>Main color card detector that manages multiple template matchers.</p> <p>Coordinates detection across multiple template types (ColorChecker 24, ColorChecker 8, checker_cm) and handles template loading with fallback resolution for package templates.</p> <p>Initialize card detector with template paths.</p> <p>Loads template images and creates TemplateMatcher instances for each valid template. Provides fallback resolution for relative paths using package template directories.</p> <p>Parameters:</p> Name Type Description Default <code>template_paths</code> <code>List[str]</code> <p>List of paths to template image files. Supports absolute paths or relative names that will be resolved against package template directories.</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If no valid templates could be loaded.</p> <code>ValueError</code> <p>If template_paths contains None values.</p> Source code in <code>src\\ascota_core\\imaging.py</code> <pre><code>def __init__(self, template_paths: List[str]) -&gt; None:\n    \"\"\"Initialize card detector with template paths.\n\n    Loads template images and creates TemplateMatcher instances for each\n    valid template. Provides fallback resolution for relative paths using\n    package template directories.\n\n    Args:\n        template_paths: List of paths to template image files. Supports\n            absolute paths or relative names that will be resolved against\n            package template directories.\n\n    Raises:\n        RuntimeError: If no valid templates could be loaded.\n        ValueError: If template_paths contains None values.\n    \"\"\"\n    self.matchers = []\n    # Try to resolve and load each template path. Provide detailed errors and debug info.\n    for p in template_paths:\n        try:\n            if p is None:\n                raise ValueError(\"Template path is None\")\n\n            # Attempt to load using provided helper. If p appears relative, try repository templates folder too.\n            img = None\n            try:\n                img = load_image_any(p)\n            except Exception:\n                # Try to resolve relative to package templates directory\n                alt = Path(__file__).parent / \"templates\" / Path(p).name\n                if alt.exists():\n                    if str(alt) != str(p) and __debug__:\n                        print(f\"DEBUG: Resolved template '{p}' -&gt; '{alt}'\")\n                    img = load_image_any(str(alt))\n                else:\n                    # Try workspace-level templates folder\n                    alt2 = Path(__file__).parents[2] / \"templates\" / Path(p).name\n                    if alt2.exists():\n                        if __debug__:\n                            print(f\"DEBUG: Resolved template '{p}' -&gt; '{alt2}'\")\n                        img = load_image_any(str(alt2))\n\n            if img is None:\n                raise FileNotFoundError(f\"Could not load template image for path '{p}' - tried provided path and known template folders\")\n\n            # Validate image loaded\n            if not isinstance(img, np.ndarray) or img.size == 0:\n                raise ValueError(f\"Template loaded from '{p}' is invalid or empty\")\n\n            self.matchers.append(TemplateMatcher(img, name=Path(p).stem))\n        except Exception as e:\n            # Surface useful debugging information and re-raise to fail fast\n            msg = f\"Failed to initialize template matcher for '{p}': {e}\"\n            print(f\"DEBUG: {msg}\")\n            raise\n</code></pre>"},{"location":"api/ascota_core/imaging/#ascota_core.imaging.CardDetector.detect","title":"detect","text":"<pre><code>detect(img_bgr, max_per_template=2, debug=False)\n</code></pre> <p>Detect color cards in image with maximum limit enforcement.</p> <p>Runs detection across all loaded template matchers and aggregates results. Enforces a maximum of 2 total cards regardless of template-specific limits and filters out cards that are too large relative to image size.</p> <p>Parameters:</p> Name Type Description Default <code>img_bgr</code> <code>ndarray</code> <p>Input image in OpenCV BGR format as numpy array.</p> required <code>max_per_template</code> <code>int</code> <p>Maximum instances per template (currently ignored, system enforces max 2 total cards).</p> <code>2</code> <code>debug</code> <code>bool</code> <p>If True, print debug information during detection process.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[ndarray]</code> <p>List of detected polygon coordinates as numpy arrays with shape (4, 2).</p> <code>List[ndarray]</code> <p>Maximum of 2 polygons will be returned, filtered by size and quality.</p> Source code in <code>src\\ascota_core\\imaging.py</code> <pre><code>def detect(self, img_bgr: np.ndarray, max_per_template: int = 2, debug: bool = False) -&gt; List[np.ndarray]:\n    \"\"\"Detect color cards in image with maximum limit enforcement.\n\n    Runs detection across all loaded template matchers and aggregates results.\n    Enforces a maximum of 2 total cards regardless of template-specific limits\n    and filters out cards that are too large relative to image size.\n\n    Args:\n        img_bgr: Input image in OpenCV BGR format as numpy array.\n        max_per_template: Maximum instances per template (currently ignored,\n            system enforces max 2 total cards).\n        debug: If True, print debug information during detection process.\n\n    Returns:\n        List of detected polygon coordinates as numpy arrays with shape (4, 2).\n        Maximum of 2 polygons will be returned, filtered by size and quality.\n    \"\"\"\n    all_polys = []\n    img_area = img_bgr.shape[0] * img_bgr.shape[1]\n    max_card_area = img_area * 0.55  # 55% of image area\n\n    for m in self.matchers:\n        polys = m.find_instances(img_bgr, max_instances=2, debug=debug)  # Max 2 per template\n\n        # Filter out cards that are too large\n        valid_polys = []\n        for poly in polys:\n            area = cv2.contourArea(poly.astype(np.float32))\n            if area &lt;= max_card_area:\n                valid_polys.append(poly)\n            else:\n                print(f\"DEBUG: Rejected card too large: {area:.0f} pixels (max: {max_card_area:.0f})\")\n\n        all_polys.extend(valid_polys)\n\n    # suppress overlaps across templates too\n    all_polys = non_max_suppression_polys(all_polys, iou_thresh=0.35)\n\n    # If we have fewer than 2 cards, try contour-based fallback\n    if len(all_polys) &lt; 2:\n        print(\"DEBUG: Using contour-based fallback for additional detection\")\n        contour_polys = self._contour_based_detection(img_bgr, all_polys, max_card_area, debug=debug)\n        all_polys.extend(contour_polys)\n        # Re-apply NMS to the combined results\n        all_polys = non_max_suppression_polys(all_polys, iou_thresh=0.35)\n\n    # Limit to maximum 2 cards total\n    if len(all_polys) &gt; 2:\n        # Sort by area and keep the 2 largest\n        areas = [cv2.contourArea(poly.astype(np.float32)) for poly in all_polys]\n        sorted_indices = np.argsort(areas)[::-1]  # Sort descending\n        all_polys = [all_polys[i] for i in sorted_indices[:2]]\n\n    # Final similarity check for debugging\n    if all_polys:\n        if debug:\n            print(f\"DEBUG: CardDetector - Final validation of {len(all_polys)} detected cards\")\n\n        # Filter out cards with low similarity\n        valid_polys = []\n        for i, poly in enumerate(all_polys):\n            # Find which template best matches this polygon\n            best_similarity = 0\n            best_template = \"unknown\"\n            similarity_scores = {}\n\n            for matcher in self.matchers:\n                try:\n                    # Extract region and compare with template\n                    x, y, w, h = cv2.boundingRect(poly.astype(np.int32))\n\n                    # Ensure coordinates are within image bounds\n                    img_h, img_w = img_bgr.shape[:2]\n                    x = max(0, min(x, img_w - 1))\n                    y = max(0, min(y, img_h - 1))\n                    w = min(w, img_w - x)\n                    h = min(h, img_h - y)\n\n                    # Check if region is valid\n                    if w &lt;= 0 or h &lt;= 0:\n                        continue\n\n                    detected_region = cv2.cvtColor(img_bgr[y:y+h, x:x+w], cv2.COLOR_BGR2GRAY)\n\n                    # Check if detected region is valid\n                    if detected_region.size == 0:\n                        continue\n\n                    template_resized = cv2.resize(matcher.gray, (w, h))\n\n                    # Calculate similarity using template matching\n                    result = cv2.matchTemplate(detected_region, template_resized, cv2.TM_CCOEFF_NORMED)\n                    similarity = float(result[0][0]) * 100\n                    similarity_scores[matcher.name] = similarity\n\n                    if similarity &gt; best_similarity:\n                        best_similarity = similarity\n                        best_template = matcher.name\n\n                except Exception as e:\n                    if debug:\n                        print(f\"DEBUG: CardDetector - Similarity check failed for card {i} with template {matcher.name}: {e}\")\n                    continue\n\n            if debug:\n                # Print detailed similarity scores for all templates\n                print(f\"DEBUG: CardDetector - Card {i+1} similarity scores:\")\n                for template_name, score in similarity_scores.items():\n                    print(f\"  {template_name}: {score:.2f}%\")\n                print(f\"  Best match: '{best_template}' with {best_similarity:.2f}% similarity\")\n\n            # Only keep cards with reasonable similarity (20%)\n            if best_similarity &gt;= 20.0:\n                valid_polys.append(poly)\n            elif debug:\n                print(f\"DEBUG: CardDetector - Rejecting card {i+1} due to low similarity: {best_similarity:.2f}%\")\n\n        # Update all_polys with filtered results\n        all_polys = valid_polys\n\n    return all_polys\n</code></pre>"},{"location":"api/ascota_core/imaging/#ascota_core.imaging.CardDetector._contour_based_detection","title":"_contour_based_detection","text":"<pre><code>_contour_based_detection(img_bgr, existing_polys, max_card_area, debug=False)\n</code></pre> <p>Contour-based fallback detection for improved edge coverage.</p> <p>Uses contour detection as a fallback method when template matching doesn't find sufficient cards. Applies edge detection and contour analysis to identify rectangular card-like regions.</p> <p>Parameters:</p> Name Type Description Default <code>img_bgr</code> <code>ndarray</code> <p>Input image in OpenCV BGR format as numpy array.</p> required <code>existing_polys</code> <code>List[ndarray]</code> <p>List of already detected polygons to avoid duplicates.</p> required <code>max_card_area</code> <code>float</code> <p>Maximum allowed area for detected cards in pixels.</p> required <code>debug</code> <code>bool</code> <p>If True, print debug information during detection.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[ndarray]</code> <p>List of additional detected polygon coordinates as numpy arrays</p> <code>List[ndarray]</code> <p>with shape (4, 2), filtered to avoid overlaps with existing detections.</p> Source code in <code>src\\ascota_core\\imaging.py</code> <pre><code>def _contour_based_detection(self, img_bgr: np.ndarray, existing_polys: List[np.ndarray], \n                           max_card_area: float, debug: bool = False) -&gt; List[np.ndarray]:\n    \"\"\"Contour-based fallback detection for improved edge coverage.\n\n    Uses contour detection as a fallback method when template matching\n    doesn't find sufficient cards. Applies edge detection and contour\n    analysis to identify rectangular card-like regions.\n\n    Args:\n        img_bgr: Input image in OpenCV BGR format as numpy array.\n        existing_polys: List of already detected polygons to avoid duplicates.\n        max_card_area: Maximum allowed area for detected cards in pixels.\n        debug: If True, print debug information during detection.\n\n    Returns:\n        List of additional detected polygon coordinates as numpy arrays\n        with shape (4, 2), filtered to avoid overlaps with existing detections.\n    \"\"\"\n    # Create mask of already detected areas\n    existing_mask = np.zeros(img_bgr.shape[:2], dtype=np.uint8)\n    for poly in existing_polys:\n        mask = polygon_to_mask(img_bgr.shape, poly)\n        existing_mask = cv2.bitwise_or(existing_mask, mask)\n\n    # Use contour fallback\n    contour_polys = contour_rect_fallback(img_bgr, existing_mask, min_area=1000)\n\n    # Filter by size and aspect ratio\n    valid_contour_polys = []\n    for poly in contour_polys:\n        area = cv2.contourArea(poly.astype(np.float32))\n        if area &gt; max_card_area:\n            continue\n\n        # Check aspect ratio (should be reasonable for color cards)\n        x, y, w, h = cv2.boundingRect(poly.astype(np.int32))\n        aspect_ratio = w / h\n        if aspect_ratio &lt; 0.2 or aspect_ratio &gt; 5.0:  # Too extreme\n            continue\n\n        # Check if it overlaps significantly with existing detections\n        poly_mask = polygon_to_mask(img_bgr.shape, poly)\n        overlap = cv2.bitwise_and(poly_mask, existing_mask)\n        overlap_ratio = np.sum(overlap &gt; 0) / np.sum(poly_mask &gt; 0)\n\n        if overlap_ratio &lt; 0.3:  # Less than 30% overlap\n            valid_contour_polys.append(poly)\n            if debug:\n                print(f\"DEBUG: Contour detection - found card: area={area:.0f}, aspect={aspect_ratio:.2f}\")\n\n    return valid_contour_polys\n</code></pre>"},{"location":"api/ascota_core/imaging/#ascota_core.imaging.CardDetector.extract_all","title":"extract_all","text":"<pre><code>extract_all(img_bgr, debug=False)\n</code></pre> <p>Extract all detected color cards without perspective warping.</p> <p>Detects cards in the image and extracts rectangular regions for each detected card. Creates a combined mask showing all detected card areas. Maximum of 2 cards will be processed.</p> <p>Parameters:</p> Name Type Description Default <code>img_bgr</code> <code>ndarray</code> <p>Input image in OpenCV BGR format as numpy array.</p> required <code>debug</code> <code>bool</code> <p>If True, print debug information during extraction.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[List[ndarray], List[ndarray], ndarray]</code> <p>Tuple containing: - polygons: List of valid polygon coordinates (max 2) - extracted_crops: List of extracted card regions as numpy arrays - combined_mask: Binary mask showing all detected card areas</p> Source code in <code>src\\ascota_core\\imaging.py</code> <pre><code>def extract_all(self, img_bgr: np.ndarray, debug: bool = False) -&gt; Tuple[List[np.ndarray], List[np.ndarray], np.ndarray]:\n    \"\"\"Extract all detected color cards without perspective warping.\n\n    Detects cards in the image and extracts rectangular regions for each\n    detected card. Creates a combined mask showing all detected card areas.\n    Maximum of 2 cards will be processed.\n\n    Args:\n        img_bgr: Input image in OpenCV BGR format as numpy array.\n        debug: If True, print debug information during extraction.\n\n    Returns:\n        Tuple containing:\n            - polygons: List of valid polygon coordinates (max 2)\n            - extracted_crops: List of extracted card regions as numpy arrays\n            - combined_mask: Binary mask showing all detected card areas\n    \"\"\"\n    polys = self.detect(img_bgr, debug=debug)\n    crops = []\n    valid_polys = []\n\n    for P in polys:\n        # Additional validation before extraction\n        if self._validate_card_polygon(P, img_bgr.shape):\n            crop = extract_rectangular_region(img_bgr, P)\n            if crop is not None:\n                crops.append(crop)\n                valid_polys.append(P)\n        else:\n            if debug:\n                print(f\"DEBUG: Rejected invalid card polygon\")\n\n    mask = np.zeros(img_bgr.shape[:2], dtype=np.uint8)\n    for P in valid_polys:\n        mask = cv2.bitwise_or(mask, polygon_to_mask(img_bgr.shape, P))\n\n    return valid_polys, crops, mask\n</code></pre>"},{"location":"api/ascota_core/imaging/#ascota_core.imaging.CardDetector._validate_card_polygon","title":"_validate_card_polygon","text":"<pre><code>_validate_card_polygon(poly, img_shape, debug=False)\n</code></pre> <p>Validate that a detected polygon represents a reasonable color card.</p> <p>Performs geometric validation checks including area constraints, aspect ratio validation, and convexity testing to ensure detected polygons correspond to actual color cards rather than false positives.</p> <p>Parameters:</p> Name Type Description Default <code>poly</code> <code>ndarray</code> <p>Polygon coordinates as numpy array with shape (4, 2).</p> required <code>img_shape</code> <code>Tuple[int, int, int]</code> <p>Image dimensions as tuple (height, width, channels).</p> required <code>debug</code> <code>bool</code> <p>If True, print validation details and failure reasons.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if polygon passes all validation checks, False otherwise.</p> Source code in <code>src\\ascota_core\\imaging.py</code> <pre><code>def _validate_card_polygon(self, poly: np.ndarray, img_shape: Tuple[int, int, int], debug: bool = False) -&gt; bool:\n    \"\"\"Validate that a detected polygon represents a reasonable color card.\n\n    Performs geometric validation checks including area constraints,\n    aspect ratio validation, and convexity testing to ensure detected\n    polygons correspond to actual color cards rather than false positives.\n\n    Args:\n        poly: Polygon coordinates as numpy array with shape (4, 2).\n        img_shape: Image dimensions as tuple (height, width, channels).\n        debug: If True, print validation details and failure reasons.\n\n    Returns:\n        True if polygon passes all validation checks, False otherwise.\n    \"\"\"\n    # Check area\n    area = cv2.contourArea(poly.astype(np.float32))\n    img_area = img_shape[0] * img_shape[1]\n\n    if area &lt; 1000 or area &gt; img_area * 0.55:  # Too small or too large\n        if debug == True:\n            print(\"DEBUG: validate_card_polygon - Too small or too large\")\n        return False\n\n    # Check if polygon is convex\n    if not cv2.isContourConvex(poly.astype(np.float32)):\n        if debug == True:\n            print(\"DEBUG: validate_card_polygon - Polygon not Convex\")\n        return False\n\n    # Check aspect ratio\n    x, y, w, h = cv2.boundingRect(poly.astype(np.int32))\n    aspect_ratio = w / h\n\n    if aspect_ratio &lt; 0.1 or aspect_ratio &gt; 10.0:  # Too extreme\n        if debug == True:\n            print(\"DEBUG: validate_card_polygon - Aspect ratio extreme\")\n        return False\n\n    # Check if polygon is roughly rectangular\n    rect_area = w * h\n    if area / rect_area &lt; 0.5:  # Too irregular\n        return False\n\n    return True\n</code></pre>"},{"location":"api/ascota_core/imaging/#ascota_core.imaging.extract_rectangular_region","title":"extract_rectangular_region","text":"<pre><code>extract_rectangular_region(img_bgr, poly_xy)\n</code></pre> <p>Extract rectangular region defined by polygon without perspective warping.</p> <p>Extracts a bounding rectangle around the detected polygon, with optional contour refinement for better edge coverage. Designed for top-down images where perspective correction is not needed.</p> <p>Parameters:</p> Name Type Description Default <code>img_bgr</code> <code>ndarray</code> <p>Input image in OpenCV BGR format as numpy array.</p> required <code>poly_xy</code> <code>ndarray</code> <p>Polygon coordinates as numpy array with shape (4, 2).</p> required <p>Returns:</p> Type Description <code>Optional[ndarray]</code> <p>Extracted rectangular region as numpy array, or None if extraction</p> <code>Optional[ndarray]</code> <p>fails or results in a region that is too small (&lt; 10x10 pixels).</p> Source code in <code>src\\ascota_core\\imaging.py</code> <pre><code>def extract_rectangular_region(img_bgr: np.ndarray, poly_xy: np.ndarray) -&gt; Optional[np.ndarray]:\n    \"\"\"Extract rectangular region defined by polygon without perspective warping.\n\n    Extracts a bounding rectangle around the detected polygon, with optional\n    contour refinement for better edge coverage. Designed for top-down images\n    where perspective correction is not needed.\n\n    Args:\n        img_bgr: Input image in OpenCV BGR format as numpy array.\n        poly_xy: Polygon coordinates as numpy array with shape (4, 2).\n\n    Returns:\n        Extracted rectangular region as numpy array, or None if extraction\n        fails or results in a region that is too small (&lt; 10x10 pixels).\n    \"\"\"\n    # Get initial bounding rectangle\n    x, y, w, h = cv2.boundingRect(poly_xy.astype(np.int32))\n\n    # Try to refine the bounding box using contour detection\n    refined_poly = _refine_polygon_with_contours(img_bgr, poly_xy, x, y, w, h)\n    if refined_poly is not None:\n        x, y, w, h = cv2.boundingRect(refined_poly.astype(np.int32))\n        print(f\"DEBUG: extract_rectangular_region - refined bounding box: ({x},{y},{w},{h})\")\n\n    # Ensure coordinates are within image bounds\n    img_h, img_w = img_bgr.shape[:2]\n    x = max(0, min(x, img_w - 1))\n    y = max(0, min(y, img_h - 1))\n    w = min(w, img_w - x)\n    h = min(h, img_h - y)\n\n    # Ensure minimum size\n    if w &lt; 10 or h &lt; 10:\n        print(f\"DEBUG: extract_rectangular_region - too small: {w}x{h}\")\n        return None\n\n    # Extract rectangular region\n    extracted = img_bgr[y:y+h, x:x+w]\n    print(f\"DEBUG: extract_rectangular_region - extracted {extracted.shape} from region ({x},{y},{w},{h})\")\n\n    return extracted\n</code></pre>"},{"location":"api/ascota_core/imaging/#ascota_core.imaging._refine_polygon_with_contours","title":"_refine_polygon_with_contours","text":"<pre><code>_refine_polygon_with_contours(img_bgr, original_poly, x, y, w, h)\n</code></pre> <p>Refine polygon boundaries using contour detection for better edge coverage.</p> <p>Attempts to improve polygon detection accuracy by performing contour detection within a padded region around the original polygon. Uses adaptive thresholding to handle varying lighting conditions and selects the contour that best matches the original polygon area.</p> <p>Parameters:</p> Name Type Description Default <code>img_bgr</code> <code>ndarray</code> <p>Input image in OpenCV BGR format as numpy array.</p> required <code>original_poly</code> <code>ndarray</code> <p>Original detected polygon coordinates with shape (4, 2).</p> required <code>x</code> <code>int</code> <p>X coordinate of bounding rectangle.</p> required <code>y</code> <code>int</code> <p>Y coordinate of bounding rectangle.</p> required <code>w</code> <code>int</code> <p>Width of bounding rectangle.</p> required <code>h</code> <code>int</code> <p>Height of bounding rectangle.</p> required <p>Returns:</p> Type Description <code>Optional[ndarray]</code> <p>Refined polygon coordinates as numpy array, or None if refinement fails</p> <code>Optional[ndarray]</code> <p>or no suitable contour is found.</p> Source code in <code>src\\ascota_core\\imaging.py</code> <pre><code>def _refine_polygon_with_contours(img_bgr: np.ndarray, original_poly: np.ndarray, \n                                 x: int, y: int, w: int, h: int) -&gt; Optional[np.ndarray]:\n    \"\"\"Refine polygon boundaries using contour detection for better edge coverage.\n\n    Attempts to improve polygon detection accuracy by performing contour detection\n    within a padded region around the original polygon. Uses adaptive thresholding\n    to handle varying lighting conditions and selects the contour that best matches\n    the original polygon area.\n\n    Args:\n        img_bgr: Input image in OpenCV BGR format as numpy array.\n        original_poly: Original detected polygon coordinates with shape (4, 2).\n        x: X coordinate of bounding rectangle.\n        y: Y coordinate of bounding rectangle.\n        w: Width of bounding rectangle.\n        h: Height of bounding rectangle.\n\n    Returns:\n        Refined polygon coordinates as numpy array, or None if refinement fails\n        or no suitable contour is found.\n    \"\"\"\n    # Extract region around the detected card\n    img_h, img_w = img_bgr.shape[:2]\n    x = max(0, x - 20)  # Add some padding\n    y = max(0, y - 20)\n    w = min(w + 40, img_w - x)\n    h = min(h + 40, img_h - y)\n\n    roi = img_bgr[y:y+h, x:x+w]\n    if roi.size == 0:\n        return None\n\n    # Convert to grayscale and apply edge detection\n    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n    gray = cv2.GaussianBlur(gray, (5, 5), 0)\n\n    # Use adaptive thresholding to handle varying lighting\n    thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n                                  cv2.THRESH_BINARY, 11, 2)\n\n    # Find contours\n    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    if not contours:\n        return None\n\n    # Find the contour closest to the original polygon area\n    original_area = cv2.contourArea(original_poly.astype(np.float32))\n    best_contour = None\n    best_area_diff = float('inf')\n\n    for contour in contours:\n        area = cv2.contourArea(contour)\n        area_diff = abs(area - original_area)\n\n        # Check if contour is roughly rectangular\n        epsilon = 0.02 * cv2.arcLength(contour, True)\n        approx = cv2.approxPolyDP(contour, epsilon, True)\n\n        if len(approx) &gt;= 4 and area_diff &lt; best_area_diff:\n            best_contour = contour\n            best_area_diff = area_diff\n\n    if best_contour is not None:\n        # Convert back to original image coordinates\n        refined_poly = best_contour.reshape(-1, 2) + np.array([x, y])\n        return refined_poly.astype(np.float32)\n\n    return None\n</code></pre>"},{"location":"api/ascota_core/imaging/#ascota_core.imaging.detect_card_type","title":"detect_card_type","text":"<pre><code>detect_card_type(num_cards_detected, debug=False)\n</code></pre> <p>Determine card types based on the number of detected cards.</p> <p>Uses a simplified rule-based approach for card type classification: - Single card detection assumes ColorChecker 8 - Dual card detection assumes ColorChecker 24 plus checker_cm scale reference</p> <p>Parameters:</p> Name Type Description Default <code>num_cards_detected</code> <code>int</code> <p>Total number of cards detected in the image.</p> required <code>debug</code> <code>bool</code> <p>If True, print debug information about type assignment.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of card type strings. For 1 card: ['colorchecker8'].</p> <code>List[str]</code> <p>For 2 cards: ['colorchecker24', 'checker_cm'] where the first</p> <code>List[str]</code> <p>entry should correspond to the larger detected card.</p> <code>List[str]</code> <p>Returns empty list for any other number of cards.</p> Source code in <code>src\\ascota_core\\imaging.py</code> <pre><code>def detect_card_type(num_cards_detected: int, debug: bool = False) -&gt; List[str]:\n    \"\"\"Determine card types based on the number of detected cards.\n\n    Uses a simplified rule-based approach for card type classification:\n    - Single card detection assumes ColorChecker 8\n    - Dual card detection assumes ColorChecker 24 plus checker_cm scale reference\n\n    Args:\n        num_cards_detected: Total number of cards detected in the image.\n        debug: If True, print debug information about type assignment.\n\n    Returns:\n        List of card type strings. For 1 card: ['colorchecker8'].\n        For 2 cards: ['colorchecker24', 'checker_cm'] where the first\n        entry should correspond to the larger detected card.\n        Returns empty list for any other number of cards.\n    \"\"\"\n    if num_cards_detected == 1:\n        if debug:\n            print(\"DEBUG: detect_card_type - 1 card detected -&gt; ColorChecker 8\")\n        return ['colorchecker8']\n    elif num_cards_detected == 2:\n        if debug:\n            print(\"DEBUG: detect_card_type - 2 cards detected -&gt; ColorChecker 24 + checker_cm\")\n        return ['colorchecker24', 'checker_cm']\n    else:\n        if debug:\n            print(f\"DEBUG: detect_card_type - {num_cards_detected} cards detected -&gt; default to ColorChecker 8\")\n        return ['colorchecker8']\n</code></pre>"},{"location":"api/ascota_core/imaging/#ascota_core.imaging._assign_card_types_by_size","title":"_assign_card_types_by_size","text":"<pre><code>_assign_card_types_by_size(polygons, debug=False)\n</code></pre> <p>Assign card types based on relative area when 2 cards are detected.</p> <p>Uses a simple size-based classification rule: the larger card is assumed to be ColorChecker 24 and the smaller card is assumed to be the checker_cm scale reference card.</p> <p>Parameters:</p> Name Type Description Default <code>polygons</code> <code>List[ndarray]</code> <p>List of polygon coordinates, expected to contain exactly 2 polygons. Each polygon should be a numpy array with shape (4, 2).</p> required <code>debug</code> <code>bool</code> <p>If True, print debug information about area calculations.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of card type strings in the same order as input polygons.</p> <code>List[str]</code> <p>Returns ['colorchecker8'] as fallback if not exactly 2 polygons provided.</p> Source code in <code>src\\ascota_core\\imaging.py</code> <pre><code>def _assign_card_types_by_size(polygons: List[np.ndarray], debug: bool = False) -&gt; List[str]:\n    \"\"\"Assign card types based on relative area when 2 cards are detected.\n\n    Uses a simple size-based classification rule: the larger card is assumed\n    to be ColorChecker 24 and the smaller card is assumed to be the checker_cm\n    scale reference card.\n\n    Args:\n        polygons: List of polygon coordinates, expected to contain exactly 2 polygons.\n            Each polygon should be a numpy array with shape (4, 2).\n        debug: If True, print debug information about area calculations.\n\n    Returns:\n        List of card type strings in the same order as input polygons.\n        Returns ['colorchecker8'] as fallback if not exactly 2 polygons provided.\n    \"\"\"\n    import cv2\n    import numpy as np\n\n    if len(polygons) != 2:\n        return ['colorchecker8']  # Fallback\n\n    # Calculate areas\n    areas = [cv2.contourArea(poly.astype(np.float32)) for poly in polygons]\n    larger_index = 0 if areas[0] &gt; areas[1] else 1\n    smaller_index = 1 - larger_index\n\n    # Create card types list in original polygon order\n    card_types = ['', '']\n    card_types[larger_index] = 'colorchecker24'\n    card_types[smaller_index] = 'checker_cm'\n\n    if debug:\n        print(f\"DEBUG: _assign_card_types_by_size - Larger card (area: {areas[larger_index]:.0f}) = ColorChecker 24, Smaller card (area: {areas[smaller_index]:.0f}) = checker_cm\")\n\n    return card_types\n</code></pre>"},{"location":"api/ascota_core/imaging/#ascota_core.imaging.create_card_masks_and_transparent","title":"create_card_masks_and_transparent","text":"<pre><code>create_card_masks_and_transparent(img_bgr, polygons, debug=False)\n</code></pre> <p>Create binary masks and transparent images for detected color cards.</p> <p>Generates binary masks where card regions are white (255) and background is black (0), then creates transparent images by applying these masks as alpha channels to the original image.</p> <p>Parameters:</p> Name Type Description Default <code>img_bgr</code> <code>ndarray</code> <p>Input image in OpenCV BGR format as numpy array.</p> required <code>polygons</code> <code>List[ndarray]</code> <p>List of detected card polygons, each with shape (4, 2).</p> required <code>debug</code> <code>bool</code> <p>If True, print debug information about mask creation.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[List[Image], List[Image]]</code> <p>Tuple containing: - masks: List of PIL Images in grayscale mode ('L') representing binary masks - transparent_images: List of PIL Images in RGBA mode with transparency applied</p> Source code in <code>src\\ascota_core\\imaging.py</code> <pre><code>def create_card_masks_and_transparent(img_bgr: np.ndarray, polygons: List[np.ndarray], \n                                     debug: bool = False) -&gt; Tuple[List[Image.Image], List[Image.Image]]:\n    \"\"\"Create binary masks and transparent images for detected color cards.\n\n    Generates binary masks where card regions are white (255) and background\n    is black (0), then creates transparent images by applying these masks\n    as alpha channels to the original image.\n\n    Args:\n        img_bgr: Input image in OpenCV BGR format as numpy array.\n        polygons: List of detected card polygons, each with shape (4, 2).\n        debug: If True, print debug information about mask creation.\n\n    Returns:\n        Tuple containing:\n            - masks: List of PIL Images in grayscale mode ('L') representing binary masks\n            - transparent_images: List of PIL Images in RGBA mode with transparency applied\n    \"\"\"\n    masks = []\n    transparent_images = []\n\n    pil_img = cv2_to_pil(img_bgr)\n\n    for poly in polygons:\n        # Create mask for this polygon\n        mask = polygon_to_mask(img_bgr.shape, poly)\n        masks.append(Image.fromarray(mask, mode='L'))\n\n        # Create transparent image\n        transparent_img = create_transparent_image(pil_img, mask)\n        transparent_images.append(transparent_img)\n\n    return masks, transparent_images\n</code></pre>"},{"location":"api/ascota_core/imaging/#ascota_core.imaging.mask_out_cards","title":"mask_out_cards","text":"<pre><code>mask_out_cards(img_bgr, polygons, fill_color=None, debug=False)\n</code></pre> <p>Mask out detected color cards by filling regions with background color.</p> <p>Creates a copy of the input image and fills the detected card regions with either a specified color or automatically calculated background color. This removes the cards from view while preserving the rest of the image.</p> <p>Parameters:</p> Name Type Description Default <code>img_bgr</code> <code>ndarray</code> <p>Input image in OpenCV BGR format as numpy array.</p> required <code>polygons</code> <code>List[ndarray]</code> <p>List of detected card polygons, each with shape (4, 2).</p> required <code>fill_color</code> <code>Optional[Tuple[int, int, int]]</code> <p>Optional BGR color tuple to fill masked regions.  If None, automatically calculates average background color.</p> <code>None</code> <code>debug</code> <code>bool</code> <p>If True, print debug information about the masking process.</p> <code>False</code> <p>Returns:</p> Type Description <code>Image</code> <p>PIL Image in RGB format with card regions filled with the specified</p> <code>Image</code> <p>or calculated background color.</p> Source code in <code>src\\ascota_core\\imaging.py</code> <pre><code>def mask_out_cards(img_bgr: np.ndarray, polygons: List[np.ndarray], \n                  fill_color: Optional[Tuple[int, int, int]] = None, debug: bool = False) -&gt; Image.Image:\n    \"\"\"Mask out detected color cards by filling regions with background color.\n\n    Creates a copy of the input image and fills the detected card regions\n    with either a specified color or automatically calculated background color.\n    This removes the cards from view while preserving the rest of the image.\n\n    Args:\n        img_bgr: Input image in OpenCV BGR format as numpy array.\n        polygons: List of detected card polygons, each with shape (4, 2).\n        fill_color: Optional BGR color tuple to fill masked regions. \n            If None, automatically calculates average background color.\n        debug: If True, print debug information about the masking process.\n\n    Returns:\n        PIL Image in RGB format with card regions filled with the specified\n        or calculated background color.\n    \"\"\"\n    img_masked = img_bgr.copy()\n\n    # If no fill color specified, calculate average background color\n    if fill_color is None:\n        fill_color = _calculate_background_color(img_bgr, polygons)\n        if debug:\n            print(f\"DEBUG: mask_out_cards - Calculated background color: {fill_color}\")\n\n    for poly in polygons:\n        mask = polygon_to_mask(img_bgr.shape, poly)\n        img_masked[mask &gt; 0] = fill_color\n\n    return cv2_to_pil(img_masked)\n</code></pre>"},{"location":"api/ascota_core/imaging/#ascota_core.imaging.crop_out_color_cards_for_rmbg","title":"crop_out_color_cards_for_rmbg","text":"<pre><code>crop_out_color_cards_for_rmbg(img_bgr, polygons, card_types, debug=False)\n</code></pre> <p>Crop out color reference cards and area above them for RMBG processing.</p> <p>Removes ColorChecker 24 and ColorChecker 8 cards along with the area above them to improve background removal processing. The checker_cm scale reference card is preserved as it may be needed for artifact measurement.</p> <p>Parameters:</p> Name Type Description Default <code>img_bgr</code> <code>ndarray</code> <p>Input image in OpenCV BGR format as numpy array.</p> required <code>polygons</code> <code>List[ndarray]</code> <p>List of detected card polygons, each with shape (4, 2).</p> required <code>card_types</code> <code>List[str]</code> <p>List of corresponding card type strings for each polygon. Expected types: 'colorchecker24', 'colorchecker8', 'checker_cm'.</p> required <code>debug</code> <code>bool</code> <p>If True, print debug information about cropping process.</p> <code>False</code> <p>Returns:</p> Type Description <code>Image</code> <p>PIL Image in RGB format with specified color cards and areas above</p> <code>Image</code> <p>them cropped out (filled with background color).</p> Source code in <code>src\\ascota_core\\imaging.py</code> <pre><code>def crop_out_color_cards_for_rmbg(img_bgr: np.ndarray, polygons: List[np.ndarray], \n                                 card_types: List[str], debug: bool = False) -&gt; Image.Image:\n    \"\"\"Crop out color reference cards and area above them for RMBG processing.\n\n    Removes ColorChecker 24 and ColorChecker 8 cards along with the area above\n    them to improve background removal processing. The checker_cm scale reference\n    card is preserved as it may be needed for artifact measurement.\n\n    Args:\n        img_bgr: Input image in OpenCV BGR format as numpy array.\n        polygons: List of detected card polygons, each with shape (4, 2).\n        card_types: List of corresponding card type strings for each polygon.\n            Expected types: 'colorchecker24', 'colorchecker8', 'checker_cm'.\n        debug: If True, print debug information about cropping process.\n\n    Returns:\n        PIL Image in RGB format with specified color cards and areas above\n        them cropped out (filled with background color).\n    \"\"\"\n    img_cropped = img_bgr.copy()\n    h, w = img_bgr.shape[:2]\n\n    # Find color cards (24 and 8) to crop out\n    color_card_polygons = []\n    for i, card_type in enumerate(card_types):\n        if card_type in ['colorchecker24', 'colorchecker8'] and i &lt; len(polygons):\n            color_card_polygons.append(polygons[i])\n            if debug:\n                print(f\"DEBUG: crop_out_color_cards_for_rmbg - Found {card_type} card to crop\")\n\n    if not color_card_polygons:\n        if debug:\n            print(\"DEBUG: crop_out_color_cards_for_rmbg - No color cards found, returning original image\")\n        return cv2_to_pil(img_cropped)\n\n    # Create mask for color cards and area above them\n    crop_mask = np.zeros((h, w), dtype=np.uint8)\n\n    for poly in color_card_polygons:\n        # Get bounding rectangle of the card\n        x, y, card_w, card_h = cv2.boundingRect(poly.astype(np.int32))\n\n        # Extend the area above the card (50% of card height above)\n        area_above_height = int(card_h * 0.5)\n        extended_y = max(0, y - area_above_height)\n        extended_h = card_h + area_above_height\n\n        # Create rectangle mask for this card and area above\n        cv2.rectangle(crop_mask, (x, extended_y), (x + card_w, y + card_h), 255, -1)\n\n        if debug:\n            print(f\"DEBUG: crop_out_color_cards_for_rmbg - Cropping card at ({x},{y}) size {card_w}x{card_h}, extended area: ({x},{extended_y}) to ({x+card_w},{y+card_h})\")\n\n    # Fill the masked areas with background color\n    background_color = _calculate_background_color(img_bgr, polygons)\n    img_cropped[crop_mask &gt; 0] = background_color\n\n    if debug:\n        print(f\"DEBUG: crop_out_color_cards_for_rmbg - Cropped out {len(color_card_polygons)} color cards and areas above them\")\n\n    return cv2_to_pil(img_cropped)\n</code></pre>"},{"location":"api/ascota_core/imaging/#ascota_core.imaging._calculate_background_color","title":"_calculate_background_color","text":"<pre><code>_calculate_background_color(img_bgr, polygons, debug=False)\n</code></pre> <p>Calculate average background color by sampling areas outside detected cards.</p> <p>Analyzes the image to determine the dominant background color by creating masks for detected cards, dilating them to avoid edge effects, and then sampling the remaining background pixels. Falls back to corner sampling if insufficient background area is available.</p> <p>Parameters:</p> Name Type Description Default <code>img_bgr</code> <code>ndarray</code> <p>Input image in OpenCV BGR format as numpy array.</p> required <code>polygons</code> <code>List[ndarray]</code> <p>List of detected card polygons to exclude from sampling.</p> required <code>debug</code> <code>bool</code> <p>If True, print debug information about sampling process.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[int, int, int]</code> <p>Average background color as BGR tuple (B, G, R) with uint8 values.</p> Source code in <code>src\\ascota_core\\imaging.py</code> <pre><code>def _calculate_background_color(img_bgr: np.ndarray, polygons: List[np.ndarray], \n                              debug: bool = False) -&gt; Tuple[int, int, int]:\n    \"\"\"Calculate average background color by sampling areas outside detected cards.\n\n    Analyzes the image to determine the dominant background color by creating\n    masks for detected cards, dilating them to avoid edge effects, and then\n    sampling the remaining background pixels. Falls back to corner sampling\n    if insufficient background area is available.\n\n    Args:\n        img_bgr: Input image in OpenCV BGR format as numpy array.\n        polygons: List of detected card polygons to exclude from sampling.\n        debug: If True, print debug information about sampling process.\n\n    Returns:\n        Average background color as BGR tuple (B, G, R) with uint8 values.\n    \"\"\"\n    # Create a mask of all detected card areas\n    card_mask = np.zeros(img_bgr.shape[:2], dtype=np.uint8)\n    for poly in polygons:\n        poly_mask = polygon_to_mask(img_bgr.shape, poly)\n        card_mask = cv2.bitwise_or(card_mask, poly_mask)\n\n    # Dilate the mask slightly to avoid sampling too close to card edges\n    kernel = np.ones((20, 20), np.uint8)\n    card_mask_dilated = cv2.dilate(card_mask, kernel, iterations=1)\n\n    # Get background pixels (areas not covered by cards)\n    background_mask = cv2.bitwise_not(card_mask_dilated)\n\n    # Sample background pixels\n    background_pixels = img_bgr[background_mask &gt; 0]\n\n    if len(background_pixels) == 0:\n        # Fallback: sample from image corners if no background pixels found\n        h, w = img_bgr.shape[:2]\n        corner_size = min(h, w) // 10  # Sample from corners\n        corners = [\n            img_bgr[:corner_size, :corner_size].reshape(-1, 3),  # Top-left\n            img_bgr[:corner_size, -corner_size:].reshape(-1, 3),  # Top-right\n            img_bgr[-corner_size:, :corner_size].reshape(-1, 3),  # Bottom-left\n            img_bgr[-corner_size:, -corner_size:].reshape(-1, 3)  # Bottom-right\n        ]\n        background_pixels = np.vstack(corners)\n        if debug:\n            print(\"DEBUG: _calculate_background_color - Using corner sampling fallback\")\n\n    # Calculate average color\n    avg_color = np.mean(background_pixels, axis=0).astype(np.uint8)\n    if debug:\n        print(f\"DEBUG: _calculate_background_color - Sampled {len(background_pixels)} background pixels\")\n\n    return tuple(avg_color)\n</code></pre>"},{"location":"api/ascota_core/imaging/#ascota_core.imaging.process_image_pipeline","title":"process_image_pipeline","text":"<pre><code>process_image_pipeline(image_path, template_paths=None, max_per_template=2, debug=False)\n</code></pre> <p>Process image to detect and extract color card information.</p> <p>Simplified pipeline that detects up to 2 color cards and classifies them using a simple rule-based approach: 1 card = ColorChecker 8, 2 cards = larger card is ColorChecker 24 and smaller is checker_cm scale reference.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>Union[str, Path]</code> <p>Path to the input image file as string or Path object.</p> required <code>template_paths</code> <code>Optional[List[str]]</code> <p>Optional list of template image paths for card detection. If None, uses default package templates for colorchecker24, checker_cm, and colorchecker8.</p> <code>None</code> <code>max_per_template</code> <code>int</code> <p>Maximum instances per template (currently ignored,  pipeline enforces max 2 total cards).</p> <code>2</code> <code>debug</code> <code>bool</code> <p>If True, print detailed debug information during processing.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing processed image data: - original_image: Original image as PIL Image object - detected_polygons: List of polygon coordinates (max 2) as numpy arrays - card_types: List of detected card type strings (max 2). Types are:   'colorchecker24', 'colorchecker8', or 'checker_cm' - masks: List of PIL mask images (max 2) for detected cards - transparent_images: List of PIL images with transparency (max 2) - masked_image: PIL Image with detected cards masked out in white - extracted_crops: List of extracted card regions as OpenCV arrays (max 2)</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If template loading or CardDetector initialization fails.</p> <code>ValueError</code> <p>If an unexpected number of cards is detected (not 1 or 2).</p> <code>FileNotFoundError</code> <p>If the input image cannot be loaded.</p> Source code in <code>src\\ascota_core\\imaging.py</code> <pre><code>def process_image_pipeline(image_path: Union[str, Path], template_paths: Optional[List[str]] = None, \n                          max_per_template: int = 2, debug: bool = False) -&gt; Dict[str, Any]:\n    \"\"\"Process image to detect and extract color card information.\n\n    Simplified pipeline that detects up to 2 color cards and classifies them using\n    a simple rule-based approach: 1 card = ColorChecker 8, 2 cards = larger card\n    is ColorChecker 24 and smaller is checker_cm scale reference.\n\n    Args:\n        image_path: Path to the input image file as string or Path object.\n        template_paths: Optional list of template image paths for card detection.\n            If None, uses default package templates for colorchecker24, checker_cm,\n            and colorchecker8.\n        max_per_template: Maximum instances per template (currently ignored, \n            pipeline enforces max 2 total cards).\n        debug: If True, print detailed debug information during processing.\n\n    Returns:\n        Dictionary containing processed image data:\n            - original_image: Original image as PIL Image object\n            - detected_polygons: List of polygon coordinates (max 2) as numpy arrays\n            - card_types: List of detected card type strings (max 2). Types are:\n              'colorchecker24', 'colorchecker8', or 'checker_cm'\n            - masks: List of PIL mask images (max 2) for detected cards\n            - transparent_images: List of PIL images with transparency (max 2)\n            - masked_image: PIL Image with detected cards masked out in white\n            - extracted_crops: List of extracted card regions as OpenCV arrays (max 2)\n\n    Raises:\n        RuntimeError: If template loading or CardDetector initialization fails.\n        ValueError: If an unexpected number of cards is detected (not 1 or 2).\n        FileNotFoundError: If the input image cannot be loaded.\n    \"\"\"\n    # Resolve package templates dir\n    templates_dir = Path(__file__).parent / \"templates\"\n    default_names = [\"colorchecker24.png\", \"checker_cm.png\", \"colorchecker8.png\"]\n\n    # If caller didn't pass templates, use package templates\n    if template_paths is None:\n        template_paths = [str(templates_dir / n) for n in default_names]\n    else:\n        # Resolve provided paths: prefer existing path, otherwise try package templates by name\n        resolved = []\n        for p in template_paths:\n            try:\n                if p is None:\n                    continue\n                pp = Path(p)\n                if pp.exists():\n                    resolved.append(str(pp))\n                else:\n                    alt = templates_dir / pp.name\n                    if alt.exists():\n                        if debug:\n                            print(f\"DEBUG: Resolved template '{p}' -&gt; '{alt}'\")\n                        resolved.append(str(alt))\n                    else:\n                        # keep original (will raise later) but avoid leading-root style defaults causing immediate imread warnings\n                        resolved.append(str(pp))\n            except Exception:\n                resolved.append(str(p))\n        template_paths = resolved\n\n    # Load image\n    img_bgr = load_image_any(image_path)\n    original_pil = cv2_to_pil(img_bgr)\n\n    # Check if the templates with the names \"checker_cm.png\", \"colorchecker8.png\", \"colorchecker24.png\" are readable\n    names = [\"checker_cm.png\", \"colorchecker8.png\", \"colorchecker24.png\"]\n    for name in names:\n        if not any(name in Path(p).name for p in template_paths):\n            if debug:\n                print(f\"DEBUG: process_image_pipeline - Warning: Template '{name}' not found in provided template paths\")\n\n    # Detect cards (max 2)\n    try:\n        detector = CardDetector(template_paths)\n    except Exception as e:\n        if debug:\n            print(f\"DEBUG: process_image_pipeline - Failed to initialize CardDetector with templates={template_paths}: {e}\")\n        # Raise a clearer error for callers\n        raise RuntimeError(f\"process_image_pipeline: failed to load templates or initialize CardDetector: {e}\") from e\n    polygons, crops, combined_mask = detector.extract_all(img_bgr, debug=debug)\n\n    # Simplified card type detection based on number of cards\n    num_cards = len(polygons)\n    if debug:\n        print(f\"DEBUG: process_image_pipeline - Detected {num_cards} cards\")\n\n    if num_cards == 1:\n        card_types = ['colorchecker8']\n        if debug:\n            print(\"DEBUG: process_image_pipeline - 1 card detected -&gt; ColorChecker 8\")\n    elif num_cards == 2:\n        # Assign types based on size: larger = ColorChecker 24, smaller = checker_cm\n        card_types = _assign_card_types_by_size(polygons)\n        if debug:\n            print(\"DEBUG: process_image_pipeline - 2 cards detected -&gt; ColorChecker 24 + checker_cm\")\n    else:\n        # Fallback for 0\n        card_types = ['colorchecker8'] if num_cards &gt; 0 else []\n        # Raise error\n        raise ValueError(f\"process_image_pipeline - {num_cards} cards detected\")\n\n    # Create masks and transparent images\n    masks, transparent_images = create_card_masks_and_transparent(img_bgr, polygons, debug=debug)\n\n    # Create masked image\n    masked_image = mask_out_cards(img_bgr, polygons, debug=debug)\n\n    return {\n        'original_image': original_pil,\n        'detected_polygons': polygons,\n        'card_types': card_types,\n        'masks': masks,\n        'transparent_images': transparent_images,\n        'masked_image': masked_image,\n        'extracted_crops': crops\n    }\n</code></pre>"},{"location":"api/ascota_core/imaging/#ascota_core.imaging.setup_rmbg_pipeline","title":"setup_rmbg_pipeline","text":"<pre><code>setup_rmbg_pipeline()\n</code></pre> <p>Setup RMBG-1.4 pipeline for background removal.</p> <p>Initializes the RMBG-1.4 transformer pipeline for automated background removal. This is a machine learning model that can segment foreground objects from background in images.</p> <p>Returns:</p> Type Description <code>Any</code> <p>Configured RMBG pipeline object, or None if transformers library</p> <code>Any</code> <p>is not available.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If transformers library is not installed.</p> Source code in <code>src\\ascota_core\\imaging.py</code> <pre><code>def setup_rmbg_pipeline() -&gt; Any:\n    \"\"\"Setup RMBG-1.4 pipeline for background removal.\n\n    Initializes the RMBG-1.4 transformer pipeline for automated background\n    removal. This is a machine learning model that can segment foreground\n    objects from background in images.\n\n    Returns:\n        Configured RMBG pipeline object, or None if transformers library\n        is not available.\n\n    Raises:\n        ImportError: If transformers library is not installed.\n    \"\"\"\n    try:\n        from transformers import pipeline\n        pipe = pipeline(\"image-segmentation\", model=\"briaai/RMBG-1.4\", trust_remote_code=True, use_fast=True)\n        return pipe\n    except ImportError:\n        raise ImportError(\"transformers library is required for RMBG pipeline. Install with: pip install transformers\")\n</code></pre>"},{"location":"api/ascota_core/imaging/#ascota_core.imaging.remove_background_and_generate_mask","title":"remove_background_and_generate_mask","text":"<pre><code>remove_background_and_generate_mask(\n    masked_pil_image, rmbg_pipe=None, cards_mask=None, cropped_for_rmbg=None, debug=False\n)\n</code></pre> <p>Apply RMBG background removal with overlap filtering and double-pass processing.</p> <p>Performs background removal using RMBG-1.4 model on the masked image. Includes overlap filtering to avoid conflicts with detected cards and applies a two-pass approach: first pass on cropped image, second pass on white background for better results.</p> <p>Parameters:</p> Name Type Description Default <code>masked_pil_image</code> <code>Image</code> <p>PIL Image with detected cards already masked out.</p> required <code>rmbg_pipe</code> <code>Optional[Any]</code> <p>Optional pre-configured RMBG pipeline. If None, creates new one.</p> <code>None</code> <code>cards_mask</code> <code>Optional[ndarray]</code> <p>Optional binary mask of card areas to avoid overlap conflicts.</p> <code>None</code> <code>cropped_for_rmbg</code> <code>Optional[Image]</code> <p>Optional cropped image with color cards removed for improved RMBG processing.</p> <code>None</code> <code>debug</code> <code>bool</code> <p>If True, print debug information during processing.</p> <code>False</code> <p>Returns:</p> Type Description <code>Image</code> <p>Tuple of (result_image, mask_image) as PIL Images where result_image</p> <code>Image</code> <p>contains the foreground object with transparent background and mask_image</p> <code>Tuple[Image, Image]</code> <p>is the binary segmentation mask.</p> Source code in <code>src\\ascota_core\\imaging.py</code> <pre><code>def remove_background_and_generate_mask(masked_pil_image: Image.Image, rmbg_pipe: Optional[Any] = None, \n                                       cards_mask: Optional[np.ndarray] = None, \n                                       cropped_for_rmbg: Optional[Image.Image] = None, \n                                       debug: bool = False) -&gt; Tuple[Image.Image, Image.Image]:\n    \"\"\"Apply RMBG background removal with overlap filtering and double-pass processing.\n\n    Performs background removal using RMBG-1.4 model on the masked image.\n    Includes overlap filtering to avoid conflicts with detected cards and\n    applies a two-pass approach: first pass on cropped image, second pass\n    on white background for better results.\n\n    Args:\n        masked_pil_image: PIL Image with detected cards already masked out.\n        rmbg_pipe: Optional pre-configured RMBG pipeline. If None, creates new one.\n        cards_mask: Optional binary mask of card areas to avoid overlap conflicts.\n        cropped_for_rmbg: Optional cropped image with color cards removed for\n            improved RMBG processing.\n        debug: If True, print debug information during processing.\n\n    Returns:\n        Tuple of (result_image, mask_image) as PIL Images where result_image\n        contains the foreground object with transparent background and mask_image\n        is the binary segmentation mask.\n    \"\"\"\n    if rmbg_pipe is None:\n        rmbg_pipe = setup_rmbg_pipeline()\n\n    # Use cropped image for RMBG if available, otherwise use masked image\n    rmbg_input = cropped_for_rmbg if cropped_for_rmbg is not None else masked_pil_image\n    if debug:\n        print(f\"DEBUG: remove_background_and_generate_mask - Using {'cropped' if cropped_for_rmbg is not None else 'masked'} image for RMBG\")\n\n    # Apply RMBG to get segmentation results\n    rmbg_results = rmbg_pipe(rmbg_input)\n\n    # Handle different RMBG output formats\n    if isinstance(rmbg_results, list) and len(rmbg_results) &gt; 0:\n        # Multiple segments detected\n        if debug:\n            print(f\"DEBUG: apply_rmbg_to_masked_image - Found {len(rmbg_results)} RMBG segments\")\n\n        # Filter out segments that overlap with cards\n        if cards_mask is not None:\n            filtered_results = _filter_rmbg_overlaps(rmbg_results, cards_mask)\n            if debug:\n                print(f\"DEBUG: apply_rmbg_to_masked_image - Filtered to {len(filtered_results)} non-overlapping segments\")\n        else:\n            filtered_results = rmbg_results\n\n        # Use the largest non-overlapping segment\n        if filtered_results:\n            best_segment = _select_best_rmbg_segment(filtered_results)\n            result_image = best_segment['image']\n            mask_image = best_segment['mask']\n        else:\n            # Fallback: use first result if no non-overlapping segments\n            if debug:\n                print(\"DEBUG: apply_rmbg_to_masked_image - No non-overlapping segments, using first result\")\n            result_image = rmbg_results[0]['image']\n            mask_image = rmbg_results[0]['mask']\n    else:\n        # Single result or different format\n        result_image = rmbg_results['image'] if isinstance(rmbg_results, dict) else rmbg_results\n        mask_image = rmbg_results['mask'] if isinstance(rmbg_results, dict) else None\n\n    # Ensure result_image is a PIL Image\n    if hasattr(result_image, \"convert\"):\n        pil_result = result_image.convert(\"RGBA\")\n    else:\n        pil_result = Image.fromarray(result_image).convert(\"RGBA\")\n\n    # Create a white background\n    white_bg = Image.new(\"RGBA\", pil_result.size, (255, 255, 255, 255))\n    # Composite the result image over the white background\n    composited = Image.alpha_composite(white_bg, pil_result)\n\n    # Run RMBG again on the composited image\n    if debug:\n        print(\"DEBUG: Running RMBG again on image composited over white background\")\n    rmbg_results2 = rmbg_pipe(composited)\n\n    # Handle output format for the second RMBG run\n    if isinstance(rmbg_results2, list) and len(rmbg_results2) &gt; 0:\n        # Use the largest segment\n        best_segment2 = _select_best_rmbg_segment(rmbg_results2)\n        final_result_image = best_segment2['image']\n        final_mask_image = best_segment2['mask']\n    elif isinstance(rmbg_results2, dict):\n        final_result_image = rmbg_results2.get('image', rmbg_results2)\n        final_mask_image = rmbg_results2.get('mask', None)\n    else:\n        final_result_image = rmbg_results2\n        final_mask_image = None\n\n    # Post-process: split cutouts, remove those touching edges or overlapping card masks\n    try:\n        final_result_image, final_mask_image = _split_and_filter_cutouts(\n            final_result_image,\n            final_mask_image,\n            cards_mask=cards_mask,\n            alpha_threshold=200\n        )\n    except Exception as e:\n        if debug:\n            print(f\"DEBUG: _split_and_filter_cutouts failed with error: {e}\")\n\n    return final_result_image, final_mask_image\n</code></pre>"},{"location":"api/ascota_core/imaging/#ascota_core.imaging._filter_rmbg_overlaps","title":"_filter_rmbg_overlaps","text":"<pre><code>_filter_rmbg_overlaps(rmbg_results, cards_mask, debug=False)\n</code></pre> <p>Filter out RMBG segments that overlap significantly with card areas.</p> <p>Analyzes each RMBG segmentation result and removes those that have substantial overlap with detected color card regions to avoid conflicting segmentations.</p> <p>Parameters:</p> Name Type Description Default <code>rmbg_results</code> <code>List[Dict[str, Any]]</code> <p>List of RMBG segmentation result dictionaries, each containing 'mask' and 'image' keys.</p> required <code>cards_mask</code> <code>ndarray</code> <p>Binary mask of card areas as numpy array where card regions are marked with non-zero values.</p> required <code>debug</code> <code>bool</code> <p>If True, print overlap statistics for each segment.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of filtered RMBG result dictionaries with card overlaps removed.</p> Source code in <code>src\\ascota_core\\imaging.py</code> <pre><code>def _filter_rmbg_overlaps(rmbg_results: List[Dict[str, Any]], cards_mask: np.ndarray, \n                         debug: bool = False) -&gt; List[Dict[str, Any]]:\n    \"\"\"Filter out RMBG segments that overlap significantly with card areas.\n\n    Analyzes each RMBG segmentation result and removes those that have\n    substantial overlap with detected color card regions to avoid\n    conflicting segmentations.\n\n    Args:\n        rmbg_results: List of RMBG segmentation result dictionaries, each\n            containing 'mask' and 'image' keys.\n        cards_mask: Binary mask of card areas as numpy array where card\n            regions are marked with non-zero values.\n        debug: If True, print overlap statistics for each segment.\n\n    Returns:\n        List of filtered RMBG result dictionaries with card overlaps removed.\n    \"\"\"\n    filtered_results = []\n\n    for i, result in enumerate(rmbg_results):\n        if 'mask' in result:\n            # Convert mask to numpy array if it's a PIL image\n            if hasattr(result['mask'], 'convert'):\n                mask_array = np.array(result['mask'].convert('L'))\n            else:\n                mask_array = result['mask']\n\n            # Ensure masks are the same size\n            if mask_array.shape != cards_mask.shape:\n                if debug:\n                    print(f\"DEBUG: _filter_rmbg_overlaps - Resizing mask {i} from {mask_array.shape} to {cards_mask.shape}\")\n                mask_array = cv2.resize(mask_array, (cards_mask.shape[1], cards_mask.shape[0]))\n\n            # Calculate overlap with cards\n            overlap = cv2.bitwise_and(mask_array, cards_mask)\n            overlap_ratio = np.sum(overlap &gt; 0) / np.sum(mask_array &gt; 0) if np.sum(mask_array &gt; 0) &gt; 0 else 0\n\n            if debug:\n                print(f\"DEBUG: _filter_rmbg_overlaps - Segment {i}: overlap ratio = {overlap_ratio:.3f}\")\n\n            # Keep segments with low overlap (less than 30%)\n            if overlap_ratio &lt; 0.3:\n                filtered_results.append(result)\n            else:\n                if debug:\n                    print(f\"DEBUG: _filter_rmbg_overlaps - Rejected segment {i} due to high overlap ({overlap_ratio:.3f})\")\n        else:\n            # If no mask available, keep the result\n            filtered_results.append(result)\n\n    return filtered_results\n</code></pre>"},{"location":"api/ascota_core/imaging/#ascota_core.imaging._select_best_rmbg_segment","title":"_select_best_rmbg_segment","text":"<pre><code>_select_best_rmbg_segment(rmbg_results, debug=False)\n</code></pre> <p>Select the best RMBG segment from multiple candidates.</p> <p>Evaluates multiple RMBG segmentation results and selects the most suitable one based on size, quality metrics, and geometric properties. Prefers larger, more centrally located segments.</p> <p>Parameters:</p> Name Type Description Default <code>rmbg_results</code> <code>List[Dict[str, Any]]</code> <p>List of filtered RMBG result dictionaries to evaluate.</p> required <code>debug</code> <code>bool</code> <p>If True, print scoring details for each candidate segment.</p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Best RMBG result dictionary containing 'mask' and 'image' keys,</p> <code>Optional[Dict[str, Any]]</code> <p>or None if no results are provided.</p> Source code in <code>src\\ascota_core\\imaging.py</code> <pre><code>def _select_best_rmbg_segment(rmbg_results: List[Dict[str, Any]], debug: bool = False) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Select the best RMBG segment from multiple candidates.\n\n    Evaluates multiple RMBG segmentation results and selects the most suitable\n    one based on size, quality metrics, and geometric properties. Prefers\n    larger, more centrally located segments.\n\n    Args:\n        rmbg_results: List of filtered RMBG result dictionaries to evaluate.\n        debug: If True, print scoring details for each candidate segment.\n\n    Returns:\n        Best RMBG result dictionary containing 'mask' and 'image' keys,\n        or None if no results are provided.\n    \"\"\"\n    if not rmbg_results:\n        return None\n\n    if len(rmbg_results) == 1:\n        return rmbg_results[0]\n\n    # Score segments based on size and quality\n    best_score = -1\n    best_result = rmbg_results[0]\n\n    for result in rmbg_results:\n        score = 0\n\n        # Size score (prefer larger segments)\n        if 'mask' in result:\n            mask_array = np.array(result['mask'].convert('L')) if hasattr(result['mask'], 'convert') else result['mask']\n            area = np.sum(mask_array &gt; 0)\n            score += area / 10000  # Normalize by 10k pixels\n\n        # Quality score (prefer segments with higher confidence if available)\n        if 'score' in result:\n            score += result['score'] * 100\n\n        if debug:\n            print(f\"DEBUG: _select_best_rmbg_segment - Segment score: {score:.3f}\")\n\n        if score &gt; best_score:\n            best_score = score\n            best_result = result\n\n    if debug:\n        print(f\"DEBUG: _select_best_rmbg_segment - Selected segment with score: {best_score:.3f}\")\n    return best_result\n</code></pre>"},{"location":"api/ascota_core/imaging/#ascota_core.imaging._split_and_filter_cutouts","title":"_split_and_filter_cutouts","text":"<pre><code>_split_and_filter_cutouts(\n    result_image, mask_image, cards_mask=None, alpha_threshold=200, debug=False\n)\n</code></pre> <p>Split foreground cutouts and filter components touching edges or overlapping cards.</p> <p>Performs connected component analysis on the segmented foreground to split multiple objects, then filters out components that touch image edges or overlap significantly with detected card areas. Also removes semi-transparent connector regions by applying alpha thresholding.</p> <p>Parameters:</p> Name Type Description Default <code>result_image</code> <code>Union[Image, ndarray]</code> <p>Segmented result as PIL Image or numpy array (RGBA or RGB).</p> required <code>mask_image</code> <code>Optional[Union[Image, ndarray]]</code> <p>Optional segmentation mask as PIL Image or numpy array. If None, mask will be derived from alpha channel.</p> required <code>cards_mask</code> <code>Optional[ndarray]</code> <p>Optional binary mask of card areas with shape (H, W). Non-zero values indicate card regions to avoid.</p> <code>None</code> <code>alpha_threshold</code> <code>int</code> <p>Alpha channel threshold (0-255) to remove semi-transparent connector regions. Defaults to 200.</p> <code>200</code> <code>debug</code> <code>bool</code> <p>If True, print detailed debug information about filtering process.</p> <code>False</code> <p>Returns:</p> Type Description <code>Image</code> <p>Tuple of (filtered_result_image, filtered_mask_image) as PIL Images</p> <code>Image</code> <p>in RGBA and grayscale ('L') modes respectively.</p> Source code in <code>src\\ascota_core\\imaging.py</code> <pre><code>def _split_and_filter_cutouts(result_image: Union[Image.Image, np.ndarray], \n                             mask_image: Optional[Union[Image.Image, np.ndarray]], \n                             cards_mask: Optional[np.ndarray] = None, \n                             alpha_threshold: int = 200, \n                             debug: bool = False) -&gt; Tuple[Image.Image, Image.Image]:\n    \"\"\"Split foreground cutouts and filter components touching edges or overlapping cards.\n\n    Performs connected component analysis on the segmented foreground to split\n    multiple objects, then filters out components that touch image edges or\n    overlap significantly with detected card areas. Also removes semi-transparent\n    connector regions by applying alpha thresholding.\n\n    Args:\n        result_image: Segmented result as PIL Image or numpy array (RGBA or RGB).\n        mask_image: Optional segmentation mask as PIL Image or numpy array.\n            If None, mask will be derived from alpha channel.\n        cards_mask: Optional binary mask of card areas with shape (H, W).\n            Non-zero values indicate card regions to avoid.\n        alpha_threshold: Alpha channel threshold (0-255) to remove semi-transparent\n            connector regions. Defaults to 200.\n        debug: If True, print detailed debug information about filtering process.\n\n    Returns:\n        Tuple of (filtered_result_image, filtered_mask_image) as PIL Images\n        in RGBA and grayscale ('L') modes respectively.\n    \"\"\"\n    # Normalize result image to RGBA PIL\n    if hasattr(result_image, \"convert\"):\n        res_pil = result_image.convert(\"RGBA\")\n    else:\n        res_pil = Image.fromarray(result_image).convert(\"RGBA\")\n\n    rgba = np.array(res_pil)\n    h, w = rgba.shape[:2]\n    alpha = rgba[:, :, 3]\n\n    # Normalize mask to ndarray if provided; otherwise derive from alpha\n    if mask_image is not None:\n        if hasattr(mask_image, \"convert\"):\n            mask_arr = np.array(mask_image.convert('L'))\n        else:\n            mask_arr = mask_image\n    else:\n        mask_arr = alpha\n\n    # Threshold to remove semi-transparent connectors\n    fg_bin = (mask_arr.astype(np.uint8) &gt; alpha_threshold).astype(np.uint8)\n\n    if np.sum(fg_bin) == 0:\n        # Nothing to keep\n        empty_alpha = np.zeros_like(alpha, dtype=np.uint8)\n        rgba[:, :, 3] = empty_alpha\n        return Image.fromarray(rgba, mode=\"RGBA\"), Image.fromarray(empty_alpha, mode='L')\n\n    # Prepare cards mask aligned to result size, if provided\n    cards_bin = None\n    if cards_mask is not None:\n        cards_arr = cards_mask\n        if hasattr(cards_arr, \"shape\"):\n            # Ensure 2D\n            if cards_arr.ndim == 3:\n                cards_arr = cards_arr[:, :, 0]\n            if cards_arr.shape[0] != h or cards_arr.shape[1] != w:\n                if debug:\n                    print(f\"DEBUG: _split_and_filter_cutouts - Resizing cards_mask from {cards_arr.shape} to {(h, w)}\")\n                cards_arr = cv2.resize(cards_arr.astype(np.uint8), (w, h), interpolation=cv2.INTER_NEAREST)\n            cards_bin = (cards_arr.astype(np.uint8) &gt; 0).astype(np.uint8)\n\n    # Connected components on foreground\n    num_labels, labels = cv2.connectedComponents(fg_bin, connectivity=8)\n\n    kept_mask = np.zeros_like(fg_bin, dtype=np.uint8)\n\n    for label_id in range(1, num_labels):\n        comp = (labels == label_id)\n\n        # Edge-touch check\n        touches_edge = comp[0, :].any() or comp[-1, :].any() or comp[:, 0].any() or comp[:, -1].any()\n        if touches_edge:\n            if debug:\n                print(f\"DEBUG: _split_and_filter_cutouts - Dropping component {label_id} touching edge\")\n            continue\n\n        # Overlap with cards check\n        if cards_bin is not None:\n            if (comp &amp; (cards_bin &gt; 0)).any():\n                if debug:\n                    print(f\"DEBUG: _split_and_filter_cutouts - Dropping component {label_id} overlapping cards mask\")\n                continue\n\n        kept_mask[comp] = 1\n\n    # Apply kept mask to alpha; remove semi-transparent connectors by thresholding\n    new_alpha = np.where(kept_mask &gt; 0, alpha, 0).astype(np.uint8)\n    rgba[:, :, 3] = new_alpha\n\n    filtered_res = Image.fromarray(rgba, mode=\"RGBA\")\n    filtered_mask = Image.fromarray((kept_mask * 255).astype(np.uint8), mode='L')\n\n    if debug:\n        print(f\"DEBUG: _split_and_filter_cutouts - Kept {np.unique(labels[kept_mask&gt;0]).size} components; removed {num_labels-1 - np.unique(labels[kept_mask&gt;0]).size}\")\n\n    return filtered_res, filtered_mask\n</code></pre>"},{"location":"api/ascota_core/scale/","title":"ascota_core.scale","text":"<p>The <code>scale</code> module handles geometric scaling tasks. It detects measurement cards in an image, computes a pixels-per-centimeter ratio, and uses this calibration to estimate the surface area of pottery sherds. These functions provide the quantitative backbone for later size normalization and comparisons.</p> <p>Artifact face area calculation using reference card for scale.</p>"},{"location":"api/ascota_core/scale/#ascota_core.scale.calculate_pp_cm_checker_cm","title":"calculate_pp_cm_checker_cm","text":"<pre><code>calculate_pp_cm_checker_cm(image, debug=False)\n</code></pre> <p>Calculate pixels per cm from checkerboard pattern squares.</p> <p>Analyzes a checkerboard image to detect white squares and calculate the pixels per centimeter scale factor, assuming each square is 1 cm\u00b2.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input image as numpy array (BGR or grayscale).</p> required <code>debug</code> <code>bool</code> <p>If True, return debug visualization showing detected squares.</p> <code>False</code> <p>Returns:</p> Type Description <code>float</code> <p>Tuple of (pixels_per_cm, debug_image). The debug_image is None if</p> <code>Optional[Image]</code> <p>debug=False, otherwise contains visualization of detected squares.</p> Source code in <code>src\\ascota_core\\scale.py</code> <pre><code>def calculate_pp_cm_checker_cm(image: np.ndarray, debug: bool = False) -&gt; Tuple[float, Optional[Image.Image]]:\n    \"\"\"Calculate pixels per cm from checkerboard pattern squares.\n\n    Analyzes a checkerboard image to detect white squares and calculate the\n    pixels per centimeter scale factor, assuming each square is 1 cm\u00b2.\n\n    Args:\n        image: Input image as numpy array (BGR or grayscale).\n        debug: If True, return debug visualization showing detected squares.\n\n    Returns:\n        Tuple of (pixels_per_cm, debug_image). The debug_image is None if\n        debug=False, otherwise contains visualization of detected squares.\n    \"\"\"\n    def _calculate_checker_square_area_checker_cm(image: np.ndarray, debug: bool = False) -&gt; Tuple[float, Optional[Image.Image]]:\n        \"\"\"\n        Calculate the average area of 'white' squares in a checker/card style image (Only works for checker_ppi styled images).\n        White squares are identified by higher mean gray intensity inside the quad\n        relative to the surrounding background, not by binary color.\n\n        Returns:\n            avg_area_pixels, optional debug PIL image\n        \"\"\"\n        # --- 1) Grayscale + local contrast normalization ---\n        if image.ndim == 3:\n            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        else:\n            gray = image.copy()\n\n        # CLAHE helps under uneven lighting; small median blur removes salt noise\n        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n        gray_eq = clahe.apply(gray)\n        gray_eq = cv2.medianBlur(gray_eq, 3)\n\n        # --- 2) Full binarization (global Otsu), and try both polarities ---\n        _, bin0 = cv2.threshold(gray_eq, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n        bin1 = cv2.bitwise_not(bin0)\n\n        H, W = gray.shape[:2]\n        img_area = float(H * W)\n\n        def _detect(binary_img: np.ndarray) -&gt; Tuple[float, List[np.ndarray], List[bool]]:\n            # Morphology to close tiny gaps in borders, then remove specks\n            k = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n            bw = cv2.morphologyEx(binary_img, cv2.MORPH_CLOSE, k, iterations=2)\n            bw = cv2.morphologyEx(bw, cv2.MORPH_OPEN, k, iterations=1)\n\n            # Use TREE to keep internal contours when there are borders/frames\n            contours, hierarchy = cv2.findContours(bw, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n            areas_white = []\n            quads = []\n            quad_is_white = []\n\n            # Dynamic area limits (tune if needed)\n            min_area = max(64.0, 0.00005 * img_area)   # 0.005% of image\n            max_area = 0.30 * img_area                 # up to 30% of image\n\n            for cnt in contours:\n                area = cv2.contourArea(cnt)\n                if area &lt; min_area or area &gt; max_area:\n                    continue\n\n                # Polygonal approximation + convexity\n                peri = cv2.arcLength(cnt, True)\n                approx = cv2.approxPolyDP(cnt, 0.03 * peri, True)\n                if len(approx) != 4 or not cv2.isContourConvex(approx):\n                    continue\n\n                # Rotated rectangle properties (handles rotated squares)\n                rect = cv2.minAreaRect(cnt)\n                (w, h) = rect[1]\n                if w == 0 or h == 0:\n                    continue\n                aspect = min(w, h) / max(w, h)\n                if aspect &lt; 0.80:  # too elongated to be a square-ish patch\n                    continue\n\n                # Rectangularity / solidity to reject jagged shapes\n                rect_area = w * h\n                if rect_area &lt;= 0:\n                    continue\n                rectangularity = area / rect_area\n                hull = cv2.convexHull(cnt)\n                hull_area = cv2.contourArea(hull)\n                solidity = area / (hull_area + 1e-6)\n\n                if rectangularity &lt; 0.70 or solidity &lt; 0.90:\n                    continue\n\n                # Classify \"white\" using mean gray inside vs outside\n                mask = np.zeros_like(gray, dtype=np.uint8)\n                cv2.drawContours(mask, [cnt], -1, 255, thickness=-1)\n                mean_inside = cv2.mean(gray, mask=mask)[0]\n\n                # Estimate local background by dilating mask a bit and subtracting inside\n                dil = cv2.dilate(mask, k, iterations=5)\n                ring = cv2.subtract(dil, mask)\n                # Fallback: if ring empty (near edges), compare to global mean\n                if cv2.countNonZero(ring) &gt; 0:\n                    mean_bg = cv2.mean(gray, mask=ring)[0]\n                else:\n                    mean_bg = float(np.mean(gray))\n\n                is_white = mean_inside &gt; mean_bg\n\n                quads.append(approx)\n                quad_is_white.append(is_white)\n                if is_white:\n                    areas_white.append(area)\n\n            avg_area = float(np.mean(areas_white)) if len(areas_white) &gt; 0 else 0.0\n            return avg_area, quads, quad_is_white\n\n        avg0, quads0, flags0 = _detect(bin0)\n        avg1, quads1, flags1 = _detect(bin1)\n\n        # Choose the polarity with more detected quads; tie-break by higher avg\n        if len(quads1) &gt; len(quads0) or (len(quads1) == len(quads0) and avg1 &gt; avg0):\n            avg_area = avg1\n            quads, flags = quads1, flags1\n            chosen_bin = bin1\n        else:\n            avg_area = avg0\n            quads, flags = quads0, flags0\n            chosen_bin = bin0\n\n        debug_image = None\n        if debug:\n            # Draw detections (green = white square, red = non-white)\n            vis = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) if image.ndim == 3 else cv2.cvtColor(cv2.cvtColor(image, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2RGB)\n            pil = Image.fromarray(vis)\n            draw = ImageDraw.Draw(pil)\n            for quad, is_white in zip(quads, flags):\n                pts = [(int(p[0][0]), int(p[0][1])) for p in quad]\n                color = \"lime\" if is_white else \"red\"\n                draw.polygon(pts, outline=color, width=2)\n            # Optional: overlay small preview of the chosen binary map in a corner\n            try:\n                small = cv2.resize(chosen_bin, (min(200, W//4), min(200, H//4)))\n                small_rgb = cv2.cvtColor(small, cv2.COLOR_GRAY2RGB)\n                small_pil = Image.fromarray(small_rgb)\n                pil.paste(small_pil, (10, 10))\n            except Exception:\n                pass\n            debug_image = pil\n\n        return avg_area, debug_image\n\n    pixels_per_cm, debug_image = _calculate_checker_square_area_checker_cm(image, debug=debug)\n\n    pp_cm = np.sqrt(pixels_per_cm)\n    if debug:\n        print(f'DEBUG: \"calculate_pp_cm_checker_cm\" - Raw value: {pixels_per_cm} | calculate_pp_cm_checker_cm')\n    # Round to nearest 10 (if 5 or more round up else round down)\n    pp_cm = math.floor(pp_cm / 10) * 10 if pp_cm % 10 &lt; 5 else math.ceil(pp_cm / 10) * 10\n\n    return pp_cm, debug_image\n</code></pre>"},{"location":"api/ascota_core/scale/#ascota_core.scale.calculate_pp_cm_colorchecker8","title":"calculate_pp_cm_colorchecker8","text":"<pre><code>calculate_pp_cm_colorchecker8(image, debug=False)\n</code></pre> <p>Calculate pixels per cm from ColorChecker 8 card reference points.</p> <p>Detects three circular reference points on a ColorChecker 8 card that form a 50mm x 20mm rectangle and calculates the pixels-per-centimeter scale factor. The reference points should form a right triangle with specific dimensional relationships.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input BGR image as numpy array.</p> required <code>debug</code> <code>bool</code> <p>If True, return debug visualization showing detected circles and selected reference points.</p> <code>False</code> <p>Returns:</p> Type Description <code>float</code> <p>Tuple of (pixels_per_cm, debug_image). Returns 0.0 for pixels_per_cm</p> <code>Optional[Image]</code> <p>if detection fails. Debug_image is None if debug=False.</p> Source code in <code>src\\ascota_core\\scale.py</code> <pre><code>def calculate_pp_cm_colorchecker8(image: np.ndarray, debug: bool = False) -&gt; Tuple[float, Optional[Image.Image]]:\n    \"\"\"Calculate pixels per cm from ColorChecker 8 card reference points.\n\n    Detects three circular reference points on a ColorChecker 8 card that form\n    a 50mm x 20mm rectangle and calculates the pixels-per-centimeter scale factor.\n    The reference points should form a right triangle with specific dimensional\n    relationships.\n\n    Args:\n        image: Input BGR image as numpy array.\n        debug: If True, return debug visualization showing detected circles\n            and selected reference points.\n\n    Returns:\n        Tuple of (pixels_per_cm, debug_image). Returns 0.0 for pixels_per_cm\n        if detection fails. Debug_image is None if debug=False.\n    \"\"\"\n\n    def find_circles(gray: np.ndarray) -&gt; np.ndarray:\n        # Contrast normalize + light blur helps Hough\n        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n        g = clahe.apply(gray)\n        g = cv2.GaussianBlur(g, (5, 5), 0)\n\n        H, W = gray.shape[:2]\n        minR = max(6, int(0.012 * min(H, W)))           # scale with image size\n        maxR = max(minR + 1, int(0.06 * min(H, W)))     # exclude very large circles\n\n        circles = cv2.HoughCircles(\n            g, cv2.HOUGH_GRADIENT, dp=1.2, minDist=int(0.08 * min(H, W)),\n            param1=120, param2=25, minRadius=minR, maxRadius=maxR\n        )\n        if circles is None:\n            return np.zeros((0, 3), dtype=np.float32)\n        return circles[0].astype(np.float32)  # shape (N, 3) columns x,y,r\n\n    def select_three_by_geometry(circles: np.ndarray) -&gt; Optional[np.ndarray]:\n        \"\"\"Select three circles that best match the expected geometry.\n\n        Args:\n            circles: Array of detected circles with shape (N, 3) for (x, y, radius).\n\n        Returns:\n            Array of shape (3, 3) for the best three circles, or None if no\n            suitable triplet is found.\n        \"\"\"\n        # Need at least 3 candidates\n        n = len(circles)\n        if n &lt; 3:\n            return None\n\n        best = None\n        best_err = 1e9\n        ratio_ms = 2.5                 # width/height = 50mm / 20mm\n        ratio_lm = math.sqrt(29) / 5   # diag/width\n\n        # Try every triplet (N is small after Hough)\n        for i in range(n - 2):\n            for j in range(i + 1, n - 1):\n                for k in range(j + 1, n):\n                    P = circles[[i, j, k], :3]  # (x, y, r)\n\n                    # Radii of the three should be similar\n                    r_med = np.median(P[:, 2])\n                    if r_med &lt;= 0:\n                        continue\n                    if np.max(np.abs(P[:, 2] - r_med)) / r_med &gt; 0.25:\n                        continue\n\n                    # Pairwise center distances\n                    C = P[:, :2]\n                    d = np.array([\n                        np.linalg.norm(C[0] - C[1]),\n                        np.linalg.norm(C[1] - C[2]),\n                        np.linalg.norm(C[0] - C[2]),\n                    ])\n                    d.sort()\n                    s, m, l = d  # s=height(2cm), m=width(5cm), l=diag(sqrt(29)cm)\n                    if s &lt; 1 or m &lt; 1:\n                        continue\n\n                    # Check expected ratios\n                    e1 = abs(m / s - ratio_ms) / ratio_ms\n                    e2 = abs(l / m - ratio_lm) / ratio_lm\n\n                    # Encourage near-right angle (approx) at the corner where height &amp; width meet\n                    angles = []\n                    for t in range(3):\n                        v1 = C[(t + 1) % 3] - C[t]\n                        v2 = C[(t + 2) % 3] - C[t]\n                        den = (np.linalg.norm(v1) * np.linalg.norm(v2)) + 1e-9\n                        cosang = float(np.clip(np.dot(v1, v2) / den, -1.0, 1.0))\n                        angles.append(math.degrees(math.acos(cosang)))\n                    ang_err = min(abs(a - 90) for a in angles)\n\n                    err = e1 + e2 + 0.01 * ang_err\n                    if err &lt; best_err:\n                        best_err = err\n                        best = P\n        return best\n\n    # --- preprocess ---\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if image.ndim == 3 else image.copy()\n\n    # Try both polarities (just in case)\n    circles = find_circles(gray)\n    if len(circles) == 0:\n        circles = find_circles(cv2.bitwise_not(gray))\n\n    chosen = select_three_by_geometry(circles)\n    debug_img = None\n    if chosen is None:\n        if debug:\n            rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) if image.ndim == 3 else \\\n                  cv2.cvtColor(cv2.cvtColor(image, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2RGB)\n            pil = Image.fromarray(rgb)\n            d = ImageDraw.Draw(pil)\n            for x, y, r in circles:\n                d.ellipse([x - r, y - r, x + r, y + r], outline=\"orange\", width=2)\n            debug_img = pil\n        return 0.0, debug_img\n\n    # Compute px/cm from width, height, and diagonal; robust aggregate via median\n    C = chosen[:, :2]\n    dists = np.array([\n        np.linalg.norm(C[0] - C[1]),\n        np.linalg.norm(C[1] - C[2]),\n        np.linalg.norm(C[0] - C[2]),\n    ])\n    s, m, l = np.sort(dists)                # s\u22482cm, m\u22485cm, l\u2248sqrt(29)\u22485.385cm\n    px_per_cm_candidates = np.array([\n        m / 5.0,\n        s / 2.0,\n        l / math.sqrt(29.0)\n    ])\n    px_per_cm = float(np.median(px_per_cm_candidates))\n\n    if debug:\n        rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) if image.ndim == 3 else \\\n              cv2.cvtColor(cv2.cvtColor(image, cv2.COLOR_GRAY2BGR), cv2.COLOR_BGR2RGB)\n        pil = Image.fromarray(rgb)\n        draw = ImageDraw.Draw(pil)\n        # Lightly draw all circle candidates\n        for x, y, r in circles:\n            draw.ellipse([x - r, y - r, x + r, y + r], outline=\"#88FFFF\", width=1)\n        # Highlight the selected triplet\n        for x, y, r in chosen:\n            draw.ellipse([x - r, y - r, x + r, y + r], outline=\"lime\", width=3)\n            draw.ellipse([x - 2, y - 2, x + 2, y + 2], fill=\"red\")\n        # Label px/cm\n        label = f\"px/cm ~ {px_per_cm:.2f}\"\n        w = 8 * len(label)\n        draw.rectangle([10, 10, 10 + w, 35], fill=(0, 0, 0, 150))\n        draw.text((14, 14), label, fill=\"white\")\n        debug_img = pil\n\n    if debug:\n        print(f'DEBUG: \"calculate_pp_cm_colorchecker8\" - Raw value: {px_per_cm} | calculate_pp_cm_colorchecker8')\n    # Round to nearest 10 (if 5 or more round up else round down)\n    px_per_cm = math.floor(px_per_cm / 10) * 10 if px_per_cm % 10 &lt; 5 else math.ceil(px_per_cm / 10) * 10\n\n    return px_per_cm, debug_img\n</code></pre>"},{"location":"api/ascota_core/scale/#ascota_core.scale.artifact_face_size","title":"artifact_face_size","text":"<pre><code>artifact_face_size(img, card_img, card_type, debug=False)\n</code></pre> <p>Calculate the face area of an artifact using a reference card for scale.</p> <p>Determines the area of an artifact image by counting non-transparent pixels and converting to square centimeters using a reference card for scale calibration. Handles both PIL Images and numpy arrays with various channel configurations.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>Union[ndarray, Image]</code> <p>Artifact image with transparent background. Can be PIL Image or numpy array. For arrays, supports RGBA (4-channel), RGB (3-channel), or grayscale (2D) formats.</p> required <code>card_img</code> <code>ndarray</code> <p>Reference card image as numpy array for scale calculation.</p> required <code>card_type</code> <code>str</code> <p>Type of reference card used for scale. Must be either 'colorchecker8' or 'checker_cm'.</p> required <code>debug</code> <code>bool</code> <p>If True, print detailed debug information about processing steps.</p> <code>False</code> <p>Returns:</p> Type Description <code>float</code> <p>Face area of the artifact in square centimeters (cm\u00b2).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If card_type is invalid, scale cannot be determined from the reference card, or if image format is unsupported.</p> Source code in <code>src\\ascota_core\\scale.py</code> <pre><code>def artifact_face_size(img: Union[np.ndarray, Image.Image], card_img: np.ndarray, \n                      card_type: str, debug: bool = False) -&gt; float:\n    \"\"\"Calculate the face area of an artifact using a reference card for scale.\n\n    Determines the area of an artifact image by counting non-transparent pixels\n    and converting to square centimeters using a reference card for scale calibration.\n    Handles both PIL Images and numpy arrays with various channel configurations.\n\n    Args:\n        img: Artifact image with transparent background. Can be PIL Image or\n            numpy array. For arrays, supports RGBA (4-channel), RGB (3-channel),\n            or grayscale (2D) formats.\n        card_img: Reference card image as numpy array for scale calculation.\n        card_type: Type of reference card used for scale. Must be either\n            'colorchecker8' or 'checker_cm'.\n        debug: If True, print detailed debug information about processing steps.\n\n    Returns:\n        Face area of the artifact in square centimeters (cm\u00b2).\n\n    Raises:\n        ValueError: If card_type is invalid, scale cannot be determined from\n            the reference card, or if image format is unsupported.\n    \"\"\"\n    if card_type == 'colorchecker8':\n        pp_cm, _ = calculate_pp_cm_colorchecker8(card_img, debug=False)\n    elif card_type == 'checker_cm':\n        pp_cm, _ = calculate_pp_cm_checker_cm(card_img, debug=False)\n    else:\n        raise ValueError(f\"Invalid card type: {card_type}\")\n\n    # Ensure we have a valid pixels per cm value\n    if pp_cm &lt;= 0:\n        raise ValueError(\"Could not determine scale from reference card\")\n\n    # Convert PIL Image to numpy array if needed\n    if hasattr(img, 'convert'):  # PIL Image\n        img_array = np.array(img)\n        if debug:\n            print(f'DEBUG: \"artifact_face_size\" - Converted PIL Image to numpy array, shape: {img_array.shape}, ndim: {img_array.ndim}')\n    else:\n        img_array = img  # Already numpy array\n        if debug:\n            print(f'DEBUG: \"artifact_face_size\" - Using existing numpy array, shape: {img_array.shape}, ndim: {img_array.ndim}')\n\n    # Handle transparent background: create binary mask for artifact pixels\n    if img_array.ndim == 3 and img_array.shape[2] == 4:\n        # RGBA image, use alpha channel as mask\n        if debug:\n            print('DEBUG: \"artifact_face_size\" - Using RGBA branch (4 channels)')\n        alpha = img_array[:, :, 3]\n        if debug:\n            print(f'DEBUG: \"artifact_face_size\" - Alpha channel stats - min: {alpha.min()}, max: {alpha.max()}, mean: {alpha.mean():.2f}')\n            print(f'DEBUG: \"artifact_face_size\" - Alpha &gt; 0 pixels: {np.sum(alpha &gt; 0)}')\n            print(f'DEBUG: \"artifact_face_size\" - Alpha &gt; 128 pixels: {np.sum(alpha &gt; 128)}')\n            print(f'DEBUG: \"artifact_face_size\" - Alpha &gt; 200 pixels: {np.sum(alpha &gt; 200)}')\n            print(f'DEBUG: \"artifact_face_size\" - Alpha &gt; 240 pixels: {np.sum(alpha &gt; 240)}')\n            print(f'DEBUG: \"artifact_face_size\" - Alpha = 255 pixels: {np.sum(alpha == 255)}')\n\n        # Try a very high threshold first, then fall back if needed\n        if np.sum(alpha &gt; 240) &gt; 0:\n            mask = (alpha &gt; 240).astype(np.uint8)  # Very strict threshold\n            if debug:\n                print('DEBUG: \"artifact_face_size\" - Using alpha &gt; 240 threshold')\n        elif np.sum(alpha &gt; 200) &gt; 0:\n            mask = (alpha &gt; 200).astype(np.uint8)  # High threshold\n            if debug:\n                print('DEBUG: \"artifact_face_size\" - Using alpha &gt; 200 threshold')\n        else:\n            # If all pixels have high alpha, try using RGB content instead\n            if debug:\n                print('DEBUG: \"artifact_face_size\" - All pixels have high alpha, using RGB content for mask')\n            rgb = img_array[:, :, :3]\n            # Create mask based on RGB content - assume black/transparent areas are background\n            gray = cv2.cvtColor(rgb, cv2.COLOR_RGB2GRAY)\n            mask = (gray &gt; 30).astype(np.uint8)  # Pixels brighter than 30\n    elif img_array.ndim == 3 and img_array.shape[2] == 3:\n        # RGB image, treat all nonzero as artifact (legacy fallback)\n        if debug:\n            print('DEBUG: \"artifact_face_size\" - Using RGB branch (3 channels)')\n        mask = (np.any(img_array &gt; 0, axis=2)).astype(np.uint8)\n    elif img_array.ndim == 2:\n        # Single channel, treat nonzero as artifact\n        if debug:\n            print('DEBUG: \"artifact_face_size\" - Using 2D array branch (grayscale)')\n        mask = (img_array &gt; 0).astype(np.uint8)\n    elif img_array.ndim == 4:\n        # RGBA image, use alpha channel as mask\n        alpha = img_array[:, :, 3]\n        if debug:\n            print(f'DEBUG: \"artifact_face_size\" - Alpha channel stats - min: {alpha.min()}, max: {alpha.max()}, mean: {alpha.mean():.2f}')\n            print(f'DEBUG: \"artifact_face_size\" - Alpha &gt; 0 pixels: {np.sum(alpha &gt; 0)}')\n            print(f'DEBUG: \"artifact_face_size\" - Alpha &gt; 128 pixels: {np.sum(alpha &gt; 128)}')\n            print(f'DEBUG: \"artifact_face_size\" - Alpha &gt; 200 pixels: {np.sum(alpha &gt; 200)}')\n            print(f'DEBUG: \"artifact_face_size\" - Alpha &gt; 240 pixels: {np.sum(alpha &gt; 240)}')\n            print(f'DEBUG: \"artifact_face_size\" - Alpha = 255 pixels: {np.sum(alpha == 255)}')\n\n        # Try a very high threshold first, then fall back if needed\n        if np.sum(alpha &gt; 240) &gt; 0:\n            mask = (alpha &gt; 240).astype(np.uint8)  # Very strict threshold\n            if debug:\n                print('DEBUG: \"artifact_face_size\" - Using alpha &gt; 240 threshold')\n        elif np.sum(alpha &gt; 200) &gt; 0:\n            mask = (alpha &gt; 200).astype(np.uint8)  # High threshold\n            if debug:\n                print('DEBUG: \"artifact_face_size\" - Using alpha &gt; 200 threshold')\n        else:\n            # If all pixels have high alpha, try using RGB content instead\n            if debug:\n                print('DEBUG: \"artifact_face_size\" - All pixels have high alpha, using RGB content for mask')\n            rgb = img_array[:, :, :3]\n            # Create mask based on RGB content - assume black/transparent areas are background\n            gray = cv2.cvtColor(rgb, cv2.COLOR_RGB2GRAY)\n            mask = (gray &gt; 30).astype(np.uint8)  # Pixels brighter than 30\n    else:\n        raise ValueError(\"Unsupported image shape for artifact image\")\n\n    # Count only the white/foreground pixels in the binary mask\n    artifact_pixels = cv2.countNonZero(mask)\n\n    # Convert pixels to cm\u00b2 using the calculated pixels per cm\n    # pp_cm is pixels per cm, so pp_cm\u00b2 is pixels per cm\u00b2\n    if debug:\n        print(f'DEBUG: \"artifact_face_size\" - Artifact pixels: {artifact_pixels}')\n    face_area_cm2 = artifact_pixels / (pp_cm * pp_cm)\n\n    return face_area_cm2\n</code></pre>"},{"location":"api/ascota_core/utils/","title":"ascota_core.utils","text":"<p>The <code>utils</code> module collects shared helper functions and low-level routines that support the other modules in <code>ascota_core</code>. These include general-purpose operations, input/output handling, and wrappers that simplify integration across the pipeline.</p> <p>Utility functions for image processing and color card detection.</p>"},{"location":"api/ascota_core/utils/#ascota_core.utils.load_image_any","title":"load_image_any","text":"<pre><code>load_image_any(path)\n</code></pre> <p>Load image from various formats using OpenCV.</p> <p>Loads images in common formats (JPG, JPEG, PNG, CR2, CR3) and returns them in OpenCV BGR format for further processing.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>Path to the image file as string or Path object.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>OpenCV image array in BGR color format with shape (height, width, 3).</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the image file cannot be loaded or does not exist.</p> Source code in <code>src\\ascota_core\\utils.py</code> <pre><code>def load_image_any(path: Union[str, Path]) -&gt; np.ndarray:\n    \"\"\"Load image from various formats using OpenCV.\n\n    Loads images in common formats (JPG, JPEG, PNG, CR2, CR3) and returns\n    them in OpenCV BGR format for further processing.\n\n    Args:\n        path: Path to the image file as string or Path object.\n\n    Returns:\n        OpenCV image array in BGR color format with shape (height, width, 3).\n\n    Raises:\n        FileNotFoundError: If the image file cannot be loaded or does not exist.\n    \"\"\"\n    img = cv2.imread(str(path), cv2.IMREAD_COLOR)\n    if img is None:\n        raise FileNotFoundError(f\"Could not load image: {path}\")\n    return img\n</code></pre>"},{"location":"api/ascota_core/utils/#ascota_core.utils.resize_max","title":"resize_max","text":"<pre><code>resize_max(img, max_side=2000)\n</code></pre> <p>Resize image to fit within maximum side length constraint.</p> <p>Proportionally resizes an image so that its largest dimension (width or height) does not exceed the specified maximum. If the image is already smaller, it remains unchanged.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray</code> <p>Input OpenCV image array.</p> required <code>max_side</code> <code>int</code> <p>Maximum allowed dimension in pixels. Defaults to 2000.</p> <code>2000</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Tuple of (resized_image, scale_factor). The scale_factor is 1.0 if no</p> <code>float</code> <p>resizing was needed, otherwise it's the ratio of new_size/original_size.</p> Source code in <code>src\\ascota_core\\utils.py</code> <pre><code>def resize_max(img: np.ndarray, max_side: int = 2000) -&gt; Tuple[np.ndarray, float]:\n    \"\"\"Resize image to fit within maximum side length constraint.\n\n    Proportionally resizes an image so that its largest dimension (width or height)\n    does not exceed the specified maximum. If the image is already smaller, it\n    remains unchanged.\n\n    Args:\n        img: Input OpenCV image array.\n        max_side: Maximum allowed dimension in pixels. Defaults to 2000.\n\n    Returns:\n        Tuple of (resized_image, scale_factor). The scale_factor is 1.0 if no\n        resizing was needed, otherwise it's the ratio of new_size/original_size.\n    \"\"\"\n    h, w = img.shape[:2]\n    s = max(h, w)\n    if s &lt;= max_side: \n        return img, 1.0\n    scale = max_side / s\n    img_small = cv2.resize(img, (int(w*scale), int(h*scale)), interpolation=cv2.INTER_AREA)\n    return img_small, scale\n</code></pre>"},{"location":"api/ascota_core/utils/#ascota_core.utils.polygon_to_mask","title":"polygon_to_mask","text":"<pre><code>polygon_to_mask(img_shape, poly_xy)\n</code></pre> <p>Convert polygon coordinates to a binary mask.</p> <p>Creates a binary mask where pixels inside the polygon are set to 255 and pixels outside are set to 0. Uses OpenCV's fillConvexPoly for efficient polygon filling.</p> <p>Parameters:</p> Name Type Description Default <code>img_shape</code> <code>Tuple[int, int, int]</code> <p>Shape tuple of the target image as (height, width, channels).</p> required <code>poly_xy</code> <code>ndarray</code> <p>Polygon vertices as numpy array with shape (N, 2) where N is the number of vertices and each row is (x, y) coordinates.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Binary mask as uint8 numpy array with shape (height, width).</p> <code>ndarray</code> <p>Pixels inside the polygon have value 255, outside pixels have value 0.</p> Source code in <code>src\\ascota_core\\utils.py</code> <pre><code>def polygon_to_mask(img_shape: Tuple[int, int, int], poly_xy: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Convert polygon coordinates to a binary mask.\n\n    Creates a binary mask where pixels inside the polygon are set to 255\n    and pixels outside are set to 0. Uses OpenCV's fillConvexPoly for\n    efficient polygon filling.\n\n    Args:\n        img_shape: Shape tuple of the target image as (height, width, channels).\n        poly_xy: Polygon vertices as numpy array with shape (N, 2) where\n            N is the number of vertices and each row is (x, y) coordinates.\n\n    Returns:\n        Binary mask as uint8 numpy array with shape (height, width).\n        Pixels inside the polygon have value 255, outside pixels have value 0.\n    \"\"\"\n    mask = np.zeros(img_shape[:2], dtype=np.uint8)\n    cv2.fillConvexPoly(mask, poly_xy.astype(np.int32), 255)\n    return mask\n</code></pre>"},{"location":"api/ascota_core/utils/#ascota_core.utils.non_max_suppression_polys","title":"non_max_suppression_polys","text":"<pre><code>non_max_suppression_polys(polys, iou_thresh=0.2)\n</code></pre> <p>Apply non-maximum suppression to remove overlapping polygons.</p> <p>Filters a list of polygons by removing those that have high intersection over union (IoU) with larger polygons. This helps eliminate duplicate detections of the same object.</p> <p>Parameters:</p> Name Type Description Default <code>polys</code> <code>List[ndarray]</code> <p>List of polygon arrays, each with shape (4, 2) representing four corner coordinates as (x, y) pairs.</p> required <code>iou_thresh</code> <code>float</code> <p>IoU threshold for suppression. Polygons with IoU above this threshold will be suppressed. Defaults to 0.2.</p> <code>0.2</code> <p>Returns:</p> Type Description <code>List[ndarray]</code> <p>List of filtered polygon arrays after non-maximum suppression.</p> <code>List[ndarray]</code> <p>Polygons are returned in order of decreasing area.</p> Source code in <code>src\\ascota_core\\utils.py</code> <pre><code>def non_max_suppression_polys(polys: List[np.ndarray], iou_thresh: float = 0.2) -&gt; List[np.ndarray]:\n    \"\"\"Apply non-maximum suppression to remove overlapping polygons.\n\n    Filters a list of polygons by removing those that have high intersection\n    over union (IoU) with larger polygons. This helps eliminate duplicate\n    detections of the same object.\n\n    Args:\n        polys: List of polygon arrays, each with shape (4, 2) representing\n            four corner coordinates as (x, y) pairs.\n        iou_thresh: IoU threshold for suppression. Polygons with IoU above\n            this threshold will be suppressed. Defaults to 0.2.\n\n    Returns:\n        List of filtered polygon arrays after non-maximum suppression.\n        Polygons are returned in order of decreasing area.\n    \"\"\"\n    if not polys: \n        return []\n\n    # use bounding boxes for IoU\n    boxes = []\n    for P in polys:\n        xs, ys = P[:,0], P[:,1]\n        boxes.append([xs.min(), ys.min(), xs.max(), ys.max()])\n    boxes = np.array(boxes, dtype=np.float32)\n\n    areas = (boxes[:,2]-boxes[:,0])*(boxes[:,3]-boxes[:,1])\n    order = np.argsort(-areas)\n    keep = []\n\n    def iou(a: np.ndarray, b: np.ndarray) -&gt; float:\n        \"\"\"Calculate intersection over union between two bounding boxes.\n\n        Args:\n            a: First bounding box as [x1, y1, x2, y2].\n            b: Second bounding box as [x1, y1, x2, y2].\n\n        Returns:\n            IoU value between 0 and 1.\n        \"\"\"\n        xx1 = max(a[0], b[0]); yy1 = max(a[1], b[1])\n        xx2 = min(a[2], b[2]); yy2 = min(a[3], b[3])\n        w = max(0, xx2-xx1); h = max(0, yy2-yy1)\n        inter = w*h\n        union = (a[2]-a[0])*(a[3]-a[1]) + (b[2]-b[0])*(b[3]-b[1]) - inter\n        return 0 if union &lt;= 0 else inter/union\n\n    while order.size &gt; 0:\n        i = order[0]\n        keep.append(i)\n        rest = order[1:]\n        rest_keep = []\n        for j in rest:\n            if iou(boxes[i], boxes[j]) &lt;= iou_thresh:\n                rest_keep.append(j)\n        order = np.array(rest_keep, dtype=int)\n    return [polys[i] for i in keep]\n</code></pre>"},{"location":"api/ascota_core/utils/#ascota_core.utils.cv2_to_pil","title":"cv2_to_pil","text":"<pre><code>cv2_to_pil(cv2_img)\n</code></pre> <p>Convert OpenCV image from BGR to PIL Image in RGB format.</p> <p>Performs color space conversion from BGR (Blue-Green-Red) used by OpenCV to RGB (Red-Green-Blue) used by PIL/Pillow and creates a PIL Image object.</p> <p>Parameters:</p> Name Type Description Default <code>cv2_img</code> <code>ndarray</code> <p>OpenCV image array in BGR format with shape (height, width, 3).</p> required <p>Returns:</p> Type Description <code>Image</code> <p>PIL Image object in RGB color format.</p> Source code in <code>src\\ascota_core\\utils.py</code> <pre><code>def cv2_to_pil(cv2_img: np.ndarray) -&gt; Image.Image:\n    \"\"\"Convert OpenCV image from BGR to PIL Image in RGB format.\n\n    Performs color space conversion from BGR (Blue-Green-Red) used by OpenCV\n    to RGB (Red-Green-Blue) used by PIL/Pillow and creates a PIL Image object.\n\n    Args:\n        cv2_img: OpenCV image array in BGR format with shape (height, width, 3).\n\n    Returns:\n        PIL Image object in RGB color format.\n    \"\"\"\n    rgb_img = cv2.cvtColor(cv2_img, cv2.COLOR_BGR2RGB)\n    return Image.fromarray(rgb_img)\n</code></pre>"},{"location":"api/ascota_core/utils/#ascota_core.utils.pil_to_cv2","title":"pil_to_cv2","text":"<pre><code>pil_to_cv2(pil_img)\n</code></pre> <p>Convert PIL Image from RGB to OpenCV image in BGR format.</p> <p>Converts a PIL Image object to a numpy array and performs color space conversion from RGB (Red-Green-Blue) to BGR (Blue-Green-Red) format used by OpenCV.</p> <p>Parameters:</p> Name Type Description Default <code>pil_img</code> <code>Image</code> <p>PIL Image object in RGB color format.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>OpenCV image array in BGR format with shape (height, width, 3).</p> Source code in <code>src\\ascota_core\\utils.py</code> <pre><code>def pil_to_cv2(pil_img: Image.Image) -&gt; np.ndarray:\n    \"\"\"Convert PIL Image from RGB to OpenCV image in BGR format.\n\n    Converts a PIL Image object to a numpy array and performs color space\n    conversion from RGB (Red-Green-Blue) to BGR (Blue-Green-Red) format\n    used by OpenCV.\n\n    Args:\n        pil_img: PIL Image object in RGB color format.\n\n    Returns:\n        OpenCV image array in BGR format with shape (height, width, 3).\n    \"\"\"\n    rgb_array = np.array(pil_img)\n    return cv2.cvtColor(rgb_array, cv2.COLOR_RGB2BGR)\n</code></pre>"},{"location":"api/ascota_core/utils/#ascota_core.utils.create_transparent_image","title":"create_transparent_image","text":"<pre><code>create_transparent_image(pil_img, mask)\n</code></pre> <p>Create transparent image by applying a binary mask as alpha channel.</p> <p>Takes a PIL Image and a binary mask, then creates a new RGBA image where the mask determines pixel transparency. White areas in the mask (255) become fully opaque, while black areas (0) become fully transparent.</p> <p>Parameters:</p> Name Type Description Default <code>pil_img</code> <code>Image</code> <p>Input PIL Image in any color mode.</p> required <code>mask</code> <code>ndarray</code> <p>Binary mask as numpy array with shape (height, width). Values should be 0 (transparent) or 255 (opaque).</p> required <p>Returns:</p> Type Description <code>Image</code> <p>PIL Image in RGBA mode with transparency applied according to the mask.</p> Source code in <code>src\\ascota_core\\utils.py</code> <pre><code>def create_transparent_image(pil_img: Image.Image, mask: np.ndarray) -&gt; Image.Image:\n    \"\"\"Create transparent image by applying a binary mask as alpha channel.\n\n    Takes a PIL Image and a binary mask, then creates a new RGBA image where\n    the mask determines pixel transparency. White areas in the mask (255) become\n    fully opaque, while black areas (0) become fully transparent.\n\n    Args:\n        pil_img: Input PIL Image in any color mode.\n        mask: Binary mask as numpy array with shape (height, width).\n            Values should be 0 (transparent) or 255 (opaque).\n\n    Returns:\n        PIL Image in RGBA mode with transparency applied according to the mask.\n    \"\"\"\n    # Convert mask to PIL Image\n    mask_pil = Image.fromarray(mask, mode='L')\n\n    # Create RGBA image\n    if pil_img.mode != 'RGBA':\n        pil_img = pil_img.convert('RGBA')\n\n    # Apply mask as alpha channel\n    pil_img.putalpha(mask_pil)\n    return pil_img\n</code></pre>"},{"location":"api/ascota_core/utils/#ascota_core.utils.contour_rect_fallback","title":"contour_rect_fallback","text":"<pre><code>contour_rect_fallback(img_bgr, tried_mask=None, min_area=4000)\n</code></pre> <p>Fallback rectangle detection using contour analysis and edge detection.</p> <p>Detects rectangular shapes using edge detection and contour approximation when other detection methods fail. Applies filtering based on area, convexity, and rectangularity to ensure quality detections.</p> <p>Parameters:</p> Name Type Description Default <code>img_bgr</code> <code>ndarray</code> <p>Input OpenCV image in BGR format.</p> required <code>tried_mask</code> <code>Optional[ndarray]</code> <p>Optional binary mask of previously attempted regions to exclude. Areas where mask &gt; 0 will be filled with white to avoid re-detection.</p> <code>None</code> <code>min_area</code> <code>int</code> <p>Minimum contour area in pixels for a valid rectangle detection. Defaults to 4000.</p> <code>4000</code> <p>Returns:</p> Type Description <code>List[ndarray]</code> <p>List of detected rectangle polygons, each as numpy array with shape (4, 2)</p> <code>List[ndarray]</code> <p>representing the four corner coordinates as (x, y) pairs.</p> Source code in <code>src\\ascota_core\\utils.py</code> <pre><code>def contour_rect_fallback(img_bgr: np.ndarray, tried_mask: Optional[np.ndarray] = None, \n                         min_area: int = 4000) -&gt; List[np.ndarray]:\n    \"\"\"Fallback rectangle detection using contour analysis and edge detection.\n\n    Detects rectangular shapes using edge detection and contour approximation\n    when other detection methods fail. Applies filtering based on area,\n    convexity, and rectangularity to ensure quality detections.\n\n    Args:\n        img_bgr: Input OpenCV image in BGR format.\n        tried_mask: Optional binary mask of previously attempted regions to exclude.\n            Areas where mask &gt; 0 will be filled with white to avoid re-detection.\n        min_area: Minimum contour area in pixels for a valid rectangle detection.\n            Defaults to 4000.\n\n    Returns:\n        List of detected rectangle polygons, each as numpy array with shape (4, 2)\n        representing the four corner coordinates as (x, y) pairs.\n    \"\"\"\n    img = img_bgr.copy()\n    if tried_mask is not None:\n        img[tried_mask&gt;0] = 255\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    gray = cv2.GaussianBlur(gray, (5,5), 0)\n    edges = cv2.Canny(gray, 50, 150)\n    edges = cv2.dilate(edges, np.ones((3,3), np.uint8), iterations=1)\n\n    cnts, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    quads = []\n    for c in cnts:\n        area = cv2.contourArea(c)\n        if area &lt; min_area: \n            continue\n        eps = 0.02 * cv2.arcLength(c, True)\n        approx = cv2.approxPolyDP(c, eps, True)\n        if len(approx) == 4 and cv2.isContourConvex(approx):\n            # rectangularity check\n            rect_area = cv2.contourArea(approx)\n            x,y,w,h = cv2.boundingRect(approx)\n            if rect_area/(w*h) &lt; 0.6:\n                continue\n            quads.append(approx.reshape(4,2).astype(np.float32))\n    return quads\n</code></pre>"}]}